{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GLOBAL IMPORTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.core import L\n",
    "\n",
    "from fastai.data.block import (\n",
    "    DataBlock,\n",
    ")\n",
    "\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "\n",
    "from fastai.text.all import (\n",
    "    defaults,\n",
    "    # file handler\n",
    "    get_text_files,\n",
    "    # tabular util\n",
    "    Tokenizer, WordTokenizer, SubwordTokenizer, Numericalize,\n",
    "    LMDataLoader, \n",
    "    # data block\n",
    "    TextBlock,\n",
    "    # model\n",
    "    AWD_LSTM,\n",
    "    # metric\n",
    "    Perplexity, accuracy,\n",
    "    # learner\n",
    "    language_model_learner,\n",
    "    # debug log\n",
    "    first, coll_repr\n",
    ")\n",
    "\n",
    "from fastai.text.core import replace_rep\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect source code with `??`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/torch-gpu/lib/python3.9/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inspect source\n",
    "# Tokenizer??\n",
    "replace_rep??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Deep Dive : RNNs\n",
    "\n",
    "Self-supervised learning is training a model using labels that are :\n",
    "- EMBEDDED in the independent variable\n",
    "- rather than requiring EXTERNAL labels\n",
    "- example : training the model to predict the next word in a text\n",
    "\n",
    "ULMFit - Universal Language Model Fine-tuning\n",
    "- Fine tuning the :\n",
    "    - (sequence based) language model prior to\n",
    "    - fine-tuning the classification model yiels BETTER results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing\n",
    "\n",
    "Predicting sentence length isn't obvious ... correlated to human breathe?\n",
    "- Sentences can be of different LENGTHS\n",
    "- Documents can be LONG\n",
    "\n",
    "Review our background with how a single categorical variable can be used as an\n",
    "indpendent variable, here's the approach we took for a single categorical var :\n",
    "- 1 - Make a list of all the possible levels of that categorical var -- vocab --\n",
    "- 2 - Replace each level with it's `index` in the -- vocab --\n",
    "- 3 - Create an `embedding matrix` for this containing a row for each item\n",
    "      .i.e for each item in the -- vocab --\n",
    "- 4 - Use this `embedding matrix` as the first layer of a neural network  \n",
    "\n",
    "```sh\n",
    "        A dedicated **embedding matrix** can take as inputs the raw -- vocab --  \n",
    "        indexes created in step 2;  \n",
    "            - this is equivalent to  \n",
    "            - but FASTER and more EFFICIENT than a matrix that takes as input  \n",
    "            one-hot-encoded vectors representing the indexes  \n",
    "```\n",
    "\n",
    "We can do the same thing ^ with TEXT!  What is new is the idea of a sequence\n",
    "\n",
    "- 1 - [ Tokenization ]\n",
    "    - convert text into a list of (depending on granularity) :  \n",
    "      - characters  \n",
    "      - substrings - (GPT, HuggingFace)  \n",
    "      - words  \n",
    "- 2 - [ Numericalization ]  \n",
    "    - -- vocab -- list hashed to an index number lookup\n",
    "- 3 - Language Model [ Data Loader Creation ]   \n",
    "    - `LMDataLoader` handles creating  \n",
    "      - `dependent` variable that is  \n",
    "      - `offset` from the `independent` by ONE `token`  \n",
    "    - Also handles details such as :  \n",
    "      - shuffling the training data so that the independent and dependent  \n",
    "        variable maintain their structure as required  \n",
    "      - latent breathe?  \n",
    "- 4 - [ Language Model Creation ]  \n",
    "    - RNN  \n",
    "      - handles INPUT lists that can be of ARBITRARY LENGTH  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Tokenization --\n",
    "\n",
    "Resolution\n",
    "\n",
    "- 1 - Character\n",
    "    - split into INDIVDUAL CHARS\n",
    "- 2 - Subword\n",
    "    - split into SMALLER parts\n",
    "    - based on most COMMONLY occuring substrings\n",
    "        - \"occasion\" => \"o\" \"c\" \"ca\" \"sion\"\n",
    "- 3 - Word\n",
    "    - apply language specific separator like 'white' space\n",
    "    - generally punctuation marks are SEPARATE tokens\n",
    "        - as opposed to totally NEW words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#100000) [Path('/Users/mton/.fastai/data/imdb/test/neg/1821_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/9487_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4604_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/2828_2.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/10890_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/3351_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/8070_2.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/1027_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/8248_3.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4290_4.txt')...],\n",
       " \"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "files = get_text_files(path, folders=['train', 'test', 'unsup'])\n",
    "txt = files[0].open().read()\n",
    "\n",
    "files, txt[:175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#121) ['Alan','Rickman','&','Emma','Thompson','give','good','performances','with','southern','/','New','Orleans','accents','in','this','detective','flick','.','It',\"'s\",'worth','seeing','for','their','scenes-','and','Rickman',\"'s\",'scene'...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "\n",
    "print(coll_repr(toks, 30))\n",
    "\n",
    "# The char '.' is terminated in a sentence, but not the '1.00' acronym\n",
    "# Tokenization logic needs to handle very subtle context\n",
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Common -- [ Prefixes ( xx ) ]\n",
    "\n",
    "xx - not common prefix, these are special tokens\n",
    "- xxbos : Beginning of stream\n",
    "    - this token indicates the model will learn it needs to \"forget\" what was  \n",
    "    said previously and focus on upcoming words\n",
    "- xxmaj : Indicates the next word begins with a capital \n",
    "  (we lower cased everything)\n",
    "- xxunk : Indicates a word is unknown\n",
    "- xxrep : !!!!! => `repeated char token` + `!` so we can count repeats as  \n",
    "  opposed to treating them as unique\n",
    "- xxwrep : for repeated words as opposed to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...] 31\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt)), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- FastAi -- [ Text Processing Rules ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/torch-gpu/lib/python3.9/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "# inspect source code\n",
    "replace_rep??\n",
    "\n",
    "# check default rules\n",
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Subword -- [Vocabulary Size]\n",
    "\n",
    "Subword tokenization provides an easy way to :\n",
    "- easily scale between character and word tokenization\n",
    "- handles EVERY human language (not just white space separated)\n",
    "    - including music and genomic sequences\n",
    "\n",
    "Vocabulary Size is a trade-off between :\n",
    "\n",
    "- Larger - fewer tokens per sentences\n",
    "    - faster training\n",
    "    - less state\n",
    "    - downside : LARGER EMBEDDING MATRIX\n",
    "        - requires MORE data to LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2000) [\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\",'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.','In Los Angeles, the alcoholic and lazy Hank Chinaski (Matt Dillon) performs a wide range of non-qualified functions just to get enough money to drink and gamble in horse races. His primary and only objective is writing and having sexy with dirty women.<br /><br />\"Factotum\" is an uninteresting, pointless and extremely boring movie about an irresponsible drunken vagrant that works a couple of days or weeks just to get enough money to buy spirits and gamble, being immediately fired due to his reckless behavior. In accordance with IMDb, this character would be the fictional alter-ego of the author Charles Bukowski, and based on this story, I will certainly never read any of his novels. Honestly, if the viewer likes this theme of alcoholic couples, better off watching the touching and heartbreaking Hector Babenco\\'s \"Ironweed\" or Marco Ferreri\\'s \"Storie di Ordinaria Follia\" that is based on the life of the same writer. My vote is four.<br /><br />Title (Brazil): \"Factotum \\x96 Sem Destino\" (\"Factotum \\x96 Without Destiny\")','This film is bundled along with \"Gli fumavano le Colt... lo chiamavano Camposanto\" and both films leave a lot to be desired in the way of their DVD prints. First, both films are very dark--occasionally making it hard to see exactly what\\'s happening. Second, neither film has subtitles and you are forced to watch a dubbed film--though \"Il Prezzo del Potere\" does seem to have a better dub. Personally, I always prefer subtitles but for the non-purists out there this isn\\'t a problem. These DVD problems, however, are not the fault of the original film makers--just the indifferent package being marketed four decades later.<br /><br />As for the film, it\\'s about the assassination of President Garfield. This is a MAJOR problem, as Van Johnson looks about as much like Garfield as Judy Garland. In no way whatsoever does he look like Garfield. He\\'s missing the beard, has the wrong hair color and style and is just not even close in any way (trust me on this, I am an American History teacher and we are paid to know these sort of things!). The real life Garfield was a Civil War general and looked like the guys on the Smith Brothers cough drop boxes. Plus, using some other actor to provide the voice for Johnson in the dubbing is just surreal. Never before or since has Van Johnson sounded quite so macho!! He was a fine actor...but certainly not a convincing general or macho president.<br /><br />In addition to the stupid casting, President Garfield\\'s death was in no way like this film. It\\'s obvious that the film makers are actually cashing in on the crazy speculation about conspiracies concerning the death of JFK, not Garfield. Garfield was shot in Washington, DC (not Dallas) by a lone gunman with severe mental problems--not a group of men with rifles. However, according to most experts, what actually killed Garfield (over two months later) were incompetent doctors--who probed and probed and probed to retrieve a bullet (to no avail) and never bothered cleaning their hands or implements in the process. In other words, like George Washington (who was basically killed by repeated bloodletting when suffering with pneumonia) he died due to malpractice. In the movie they got nothing right whatsoever...other than indeed President Garfield was shot.<br /><br />Because the film bears almost no similarity to real history, it\\'s like a history lesson as taught from someone from another planet or someone with a severe brain injury. Why not also include ninjas, fighting robots and the Greek gods while you\\'re at it?!?! Aside from some decent acting and production values, because the script is utter cow crap, I don\\'t recommend anyone watch it. It\\'s just a complete and utter mess.',\"I only comment on really very good films and on utter rubbish. My aim is to help people who want to see great films to spend their time - and money - wisely.<br /><br />I also want to stop people wasting their time on garbage, and want to publicize the fact that the director/producer of these garbage films can't get away with it for very long. We will find out who you are and will vote with out feet - and wallets.<br /><br />This film clearly falls into the garbage category.<br /><br />The director and writer is John Shiban. It's always a bad sign when the writer is also the director. Maybe he wants two pay cheques. He shouldn't get any. So remember the name - John SHIBAN. And if you see anything else by him, forget it.<br /><br />I won't say anything about the plot - others have already. I am a little worried by how much the director likes to zoom in to the poor girl's face when she is crying and screaming. These long duration shots are a little worrying and may say something about the state of mind of Mr. Shiban. Maybe he should get psychiatric help.<br /><br />Enough already. It's crap - don't waste your time on it.\",'When you look at the cover and read stuff about it an entirely different type of movie comes to mind than what you get here. Then again maybe I read the summary for the other movie called \"Mausolem\" instead as there were two movies of this title released about the same time with both featuring plots that had key elements in common. However, reading stuff about that movie here I know I saw this one and not that one and that movie is even less what one would imagine a movie with that title would be about. I will be honest, I expect more of a zombie type picture and you get that in this movie to some degree. However, there is more stuff involving the occult and strange powers as the opening scene of the people being taken away by the coroner at the beginning of the film will attest to. The movie also has the old theme of kids going somewhere they do not belong to have some crazy party, in this case it is in fact a mausoleum. The other movie I do not think really has that key feature playing that prominent role in the movie and I see the score for this one is higher too, still it was just not the movie I was expecting.',\"Rollerskating vampires?! I'm sorry but even for the 80's that's just way too cheesy to be remotely scary... You can excuse the original the odd kitsch moment because it was parodying old movies and TV shows, but that's been done once, so the sequel needed to be a little less camp, not even more outlandish! Plus, the first movie had the presence of Chris Sarandon - a man who could even make stalking discotheques in casual knitwear seem seductive! - that this one sorely lacks, so there was no 'danger' in anything that happened, it just seemed silly.<br /><br />Admittedly I only saw this once when I was 7, but by then already being a huge fan of the original I remember being disgusted. To me, there is no sequel to Fright Night, just a tacky spoof that doesn't deserve any appraisal whatsoever.\",'Technically abominable (with audible \"pops\" between scenes)and awesomely amateurish, \"Flesh\" requires a lot of patience to sit through and will probably turn off most viewers; but the dialogue rings amazingly true and Joe Dallesandro, who exposes his body in almost every scene, also gives an utterly convincing performance. A curio, to be sure, but the more polished \"Trash\", made two years later, is a definite step forward. I suggest you watch that instead. (*1/2)','When Hollywood is trying to grasp what an \"intelligent person\" is like, they fail so miserably, finding it hard putting words in the mouth of the purported \"genius\".<br /><br />Right, any genius walks around trying to rub in his superiority at every instance. Sure, they hang out in bars and pick fights \\x96 it\\'s not like they are (generalizing wildly) autistic nerds who never have a tan.<br /><br />Plus, if you are a genius you know all about Math and History and Politics and of course you\\'re constantly up to date with current events and a thorough analysis of them. Coz these things, like, all go together n stuff, y\\'know?<br /><br />Plus, you walk around with a smirk all the time. You are just a smug son of a you-know-what, that\\'s how it is, y\\'all. <br /><br />And of course you smoke, like someone who never smoked before, but you smoke coz it\\'s like cool n stuff, y\\'know. And you\\'re different. That is understood.<br /><br />And of course you can fight \\x96 you\\'re a bully. A bully who finds time to study 10.000 books whenever he doesn\\'t lift weights. And whenever he doesn\\'t smoke or drink beer because he follows a strict health regimen.<br /><br />And you date a 30-something college student \\x96 Minnie Driver. Well, I won\\'t even comment Matt Damon. Team America has hit the nail on the head already.<br /><br />This movie is a daydream of a Beavis & Butthead type student (in other words 95% of them): \"Yeah, that\\'s what I would be like if I was a genius.\" But stupid people and stupid authors in this case cannot imagine the lives of geniuses.','Respected western auteur Budd Boetticher is woefully out of place with this choppy modern day cops and robbers story that suffers from a strong lack of emotional believability. Boetticher seems to have waived rehearsal time and settled for the first take as leads Joe Cotton and Rhonda Fleming put little effort into their roles, delivering lines flatly and without energy. <br /><br />Mild mannered employee Leon \"Foggy\" Poole works as an inside man on a bank job that goes bad and gets his wife killed in the process. He escapes from prison and immediately sets out to kill the wife of the detective who killed his. Hundreds of cops are mobilized to keep him from getting to the home of the intended who has been moved to another location but wouldn\\'t you know in the films final moments we have Foggy trailing feet behind the victim (who thought somehow that taking a bus back to the house was a sound move) while a company of cops observe and bicker over what action to take. Sound preposterous? You should see it. It\\'s all of that and more. <br /><br />Lucien Ballard\\'s camera work does a decent job of bringing noir to the suburbs but the editing is lackadaisical and shapeless and it drains the film of its suspense and pace. As Poole, Wendell Corey is the best thing in the film managing to evoke great sympathy as he transitions from gentle soul to murderer. These attributes aside Killer uniformly fails in construction and execution making its message clear. Go Western old Budd.'...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:2000])\n",
    "txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tmp/texts.out --vocab_size=1000 --model_prefix=tmp/spm --character_coverage=0.99999 --model_type=unigram --unk_id=9 --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2 --user_defined_symbols=▁xxunk,▁xxpad,▁xxbos,▁xxeos,▁xxfld,▁xxrep,▁xxwrep,▁xxup,▁xxmaj --hard_vocab_limit=false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'▁A l an ▁R ick man ▁ & ▁E mm a ▁Th om p son ▁give ▁good ▁performance s ▁with ▁so u ther n / N e w ▁O r le an s ▁a c cent s ▁in ▁this ▁de'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁A l an ▁ R i ck m an ▁ & ▁ E m m a ▁ T h o m p s on ▁g i ve ▁g o o d ▁p er f or m an ce s ▁with'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"▁Alan ▁Rick man ▁ & ▁Emma ▁Thompson ▁give ▁good ▁performances ▁with ▁southern / N e w ▁O rleans ▁accents ▁in ▁this ▁detective ▁flick . ▁It ' s ▁worth ▁seeing ▁for ▁their ▁scenes - ▁and ▁Rick man ' s ▁scene ▁with\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numericalization with Fastai\n",
    "\n",
    "Numericalization is the process of mapping tokens to integers\n",
    "- 1 - vocabulary : list of all possible levels of categorical variable\n",
    "    - RGB captures visual pixel values!\n",
    "- 2 - replace each level with it's index in the vocab\n",
    "    - each R, G, B channel has value between 0 - 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson','give','good','performances','with','southern','/','xxmaj','new','xxmaj','orleans','accents','in','this','detective','flick','.','xxmaj','it',\"'s\",'worth','seeing'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1984) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','to','of','i','it','is','in'...]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,    0,    8, 1442,  234,    8,    0,    8,    0,  199,\n",
       "              64,  731,   29,    0,  122,    8,  253,    8,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj rickman & xxmaj xxunk xxmaj xxunk give good performances with xxunk / xxmaj new xxmaj xxunk'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting Our Texts into Batches for a Language Model\n",
    "\n",
    "With IMAGEs for batching we needed to :\n",
    "\n",
    "- RESIZE height and width\n",
    "- so we could group and stack them in a single tensor\n",
    "\n",
    "With TEXT\n",
    "- can't resize arbitrarily varied length to fix length\n",
    "- char order matters to predict next token\n",
    "- each NEW batch MUST begin precisely where the old batch finished\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "\n",
    "dl = LMDataLoader(nums200)\n",
    "\n",
    "x,y = first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj rickman & xxmaj xxunk xxmaj xxunk give good performances with xxunk / xxmaj new xxmaj xxunk'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj xxunk xxmaj rickman & xxmaj xxunk xxmaj xxunk give good performances with xxunk / xxmaj new xxmaj xxunk accents'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Training a Text Classifier --\n",
    "\n",
    "- 1 - fine-tune our `language model` trained on Wikipedia\n",
    "- 2 - use that model to train our `classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Using DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(\n",
    "    get_text_files,\n",
    "    folders = ['train', 'test', 'unsup']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: \n",
    "# '.fastai/data/imdb_tok/counter.pkl'\n",
    "# This can occur if we cancel this tok process : \n",
    "# - it'll cache in a malformed\n",
    "# - when we rerun, it only checks that the 'imdb_tok' folder exists\n",
    "# - and tries to load the `counter.pkl` that never actually got completed\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, \n",
    "    splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i loved xxmaj darkwing xxmaj duck when i was 5 years old and i still love it now ! xxmaj it 's interesting how most cartoons from childhood seem lame when i watch them now but xxmaj darkwing xxmaj duck remains funny and entertaining . xxmaj the witty dialog , action , diverse characters , and style made this show unique from most xxmaj disney cartoons . xxmaj if they ever remade the series i would most definitely watch</td>\n",
       "      <td>i loved xxmaj darkwing xxmaj duck when i was 5 years old and i still love it now ! xxmaj it 's interesting how most cartoons from childhood seem lame when i watch them now but xxmaj darkwing xxmaj duck remains funny and entertaining . xxmaj the witty dialog , action , diverse characters , and style made this show unique from most xxmaj disney cartoons . xxmaj if they ever remade the series i would most definitely watch it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sincerity . xxmaj and i got ta admit that even though xxmaj i 'm quite a huge fan of xxmaj uwe xxmaj boll 's work , i personally was n't 100 % sure either what to expect from a project like this … \\n\\n xxmaj but now that i finally got my hands on a xxup dvd of said film and ultimately got to see xxup tunnel xxup rats ( i missed the flick 's short theatrical run here in</td>\n",
       "      <td>. xxmaj and i got ta admit that even though xxmaj i 'm quite a huge fan of xxmaj uwe xxmaj boll 's work , i personally was n't 100 % sure either what to expect from a project like this … \\n\\n xxmaj but now that i finally got my hands on a xxup dvd of said film and ultimately got to see xxup tunnel xxup rats ( i missed the flick 's short theatrical run here in xxmaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [105070592/105067061 00:03&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm,\n",
    "    AWD_LSTM,\n",
    "    drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]\n",
    ").to_fp16()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
