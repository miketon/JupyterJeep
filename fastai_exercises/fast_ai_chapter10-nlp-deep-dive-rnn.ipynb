{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GLOBAL IMPORTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1 error fix\n",
    "import sentencepiece\n",
    "\n",
    "from fastai.data.external import (\n",
    "    untar_data, \n",
    "    URLs,\n",
    ")\n",
    "\n",
    "from fastai.data.core import L\n",
    "\n",
    "from fastai.data.block import (\n",
    "    DataBlock,\n",
    ")\n",
    "\n",
    "from fastai.data.transforms import (\n",
    "    RandomSplitter, \n",
    "    parent_label,\n",
    "    GrandparentSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "from fastai.text.all import (\n",
    "    defaults,\n",
    "    # file handler\n",
    "    get_text_files, parent_label,\n",
    "    # tabular util\n",
    "    Tokenizer, WordTokenizer, SubwordTokenizer, Numericalize,\n",
    "    LMDataLoader, \n",
    "    # data block\n",
    "    TextBlock, CategoryBlock,\n",
    "    # model\n",
    "    AWD_LSTM,\n",
    "    # metric\n",
    "    Perplexity, accuracy,\n",
    "    # learner\n",
    "    language_model_learner,\n",
    "    # debug log\n",
    "    first, coll_repr\n",
    ")\n",
    "\n",
    "from fastai.text.core import replace_rep\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# suppress Cuda device warnings so we don't BLOW UP logs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19687) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastbook\n",
      "  Downloading fastbook-0.0.29-py3-none-any.whl (719 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pip in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (23.3.1)\n",
      "Requirement already satisfied: packaging in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (23.1)\n",
      "Requirement already satisfied: fastai>=2.6 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (2.7.13)\n",
      "Requirement already satisfied: graphviz in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (0.20.1)\n",
      "Requirement already satisfied: pandas in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (2.1.1)\n",
      "Requirement already satisfied: requests in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (2.31.0)\n",
      "Requirement already satisfied: transformers in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (4.35.0)\n",
      "Collecting datasets (from fastbook)\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting ipywidgets<8 (from fastbook)\n",
      "  Downloading ipywidgets-7.8.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: sentencepiece in /Users/mton/.local/lib/python3.10/site-packages (from fastbook) (0.1.99)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.5.29)\n",
      "Requirement already satisfied: torchvision>=0.11 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (0.15.2a0)\n",
      "Requirement already satisfied: matplotlib in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (3.8.0)\n",
      "Requirement already satisfied: pyyaml in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (6.0.1)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (10.0.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.11.3)\n",
      "Requirement already satisfied: spacy<4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (3.7.2)\n",
      "Requirement already satisfied: torch<2.2,>=1.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (2.0.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipywidgets<8->fastbook) (0.1.4)\n",
      "Collecting ipython-genutils~=0.2.0 (from ipywidgets<8->fastbook)\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipywidgets<8->fastbook) (5.13.0)\n",
      "Collecting widgetsnbextension~=3.6.6 (from ipywidgets<8->fastbook)\n",
      "  Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipywidgets<8->fastbook) (8.17.2)\n",
      "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets<8->fastbook)\n",
      "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from datasets->fastbook) (1.26.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->fastbook)\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->fastbook)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets->fastbook)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from datasets->fastbook) (4.65.0)\n",
      "Collecting xxhash (from datasets->fastbook)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->fastbook)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->fastbook) (2023.10.0)\n",
      "Collecting aiohttp (from datasets->fastbook)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.18.0 (from datasets->fastbook)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pandas->fastbook) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pandas->fastbook) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pandas->fastbook) (2023.3)\n",
      "Requirement already satisfied: filelock in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (0.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets->fastbook) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->fastbook)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->fastbook)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->fastbook)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->fastbook)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->fastbook)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets->fastbook) (4.8.0)\n",
      "Requirement already satisfied: decorator in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->fastbook) (1.16.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.0.7)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (68.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.3.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers->fastbook)\n",
      "  Downloading tokenizers-0.14.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting transformers (from fastbook)\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers->fastbook)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: sympy in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (3.1)\n",
      "Collecting notebook>=4.4.1 (from widgetsnbextension~=3.6.6->ipywidgets<8->fastbook)\n",
      "  Downloading notebook-7.0.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from scikit-learn->fastai>=2.6->fastbook) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from scikit-learn->fastai>=2.6->fastbook) (2.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8->fastbook) (0.8.3)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.25.0)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (4.0.8)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (6.3.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8->fastbook) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.0->ipywidgets<8->fastbook) (0.2.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (2.10.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai>=2.6->fastbook) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai>=2.6->fastbook) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai>=2.6->fastbook) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai>=2.6->fastbook) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jinja2->spacy<4->fastai>=2.6->fastbook) (2.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8->fastbook) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8->fastbook) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8->fastbook) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from sympy->torch<2.2,>=1.10->fastai>=2.6->fastbook) (1.3.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (3.7.1)\n",
      "Requirement already satisfied: argon2-cffi in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (5.5.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (7.11.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (5.9.2)\n",
      "Requirement already satisfied: overrides in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.18.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (25.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.6.4)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (6.26.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.2.0)\n",
      "Requirement already satisfied: tomli in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.13.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (4.19.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.12.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (3.11.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.2.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.18.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (21.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.5.8)\n",
      "Requirement already satisfied: psutil in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (5.9.0)\n",
      "Requirement already satisfied: webencodings in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.4)\n",
      "Requirement already satisfied: uri-template in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.5)\n",
      "Requirement already satisfied: pycparser in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.8.19.14)\n",
      "Downloading ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp310-cp310-macosx_11_0_arm64.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.5/386.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading notebook-7.0.6-py3-none-any.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ipython-genutils, xxhash, pyarrow-hotfix, pyarrow, multidict, jupyterlab-widgets, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets, notebook, widgetsnbextension, ipywidgets, fastbook\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab-widgets 3.0.9\n",
      "    Uninstalling jupyterlab-widgets-3.0.9:\n",
      "      Successfully uninstalled jupyterlab-widgets-3.0.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.0\n",
      "    Uninstalling transformers-4.35.0:\n",
      "      Successfully uninstalled transformers-4.35.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.9\n",
      "    Uninstalling widgetsnbextension-4.0.9:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.9\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.0.4\n",
      "    Uninstalling ipywidgets-8.0.4:\n",
      "      Successfully uninstalled ipywidgets-8.0.4\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.15.0 dill-0.3.7 fastbook-0.0.29 frozenlist-1.4.1 huggingface-hub-0.19.4 ipython-genutils-0.2.0 ipywidgets-7.8.1 jupyterlab-widgets-1.1.7 multidict-6.0.4 multiprocess-0.70.15 notebook-7.0.6 pyarrow-14.0.1 pyarrow-hotfix-0.6 tokenizers-0.15.0 transformers-4.36.1 widgetsnbextension-3.6.6 xxhash-3.4.1 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect source code with `??`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "\n",
    "# inspect source\n",
    "# Tokenizer??\n",
    "replace_rep??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Deep Dive : RNNs\n",
    "\n",
    "Self-supervised learning is training a model using labels that are :\n",
    "- EMBEDDED in the independent variable\n",
    "- rather than requiring EXTERNAL labels\n",
    "- example : training the model to predict the next word in a text\n",
    "\n",
    "ULMFit - Universal Language Model Fine-tuning\n",
    "- Fine tuning the :\n",
    "    - (sequence based) language model prior to\n",
    "    - fine-tuning the classification model yiels BETTER results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing\n",
    "\n",
    "Predicting sentence length isn't obvious ... correlated to human breathe?\n",
    "- Sentences can be of different LENGTHS\n",
    "- Documents can be LONG\n",
    "\n",
    "Review our background with how a single categorical variable can be used as an\n",
    "indpendent variable, here's the approach we took for a single categorical var :\n",
    "- 1 - Make a list of all the possible levels of that categorical var -- vocab --\n",
    "- 2 - Replace each level with it's `index` in the -- vocab --\n",
    "- 3 - Create an `embedding matrix` for this containing a row for each item\n",
    "      .i.e for each item in the -- vocab --\n",
    "- 4 - Use this `embedding matrix` as the first layer of a neural network  \n",
    "\n",
    "```sh\n",
    "        A dedicated **embedding matrix** can take as inputs the raw -- vocab --  \n",
    "        indexes created in step 2;  \n",
    "            - this is equivalent to  \n",
    "            - but FASTER and more EFFICIENT than a matrix that takes as input  \n",
    "            one-hot-encoded vectors representing the indexes  \n",
    "```\n",
    "\n",
    "We can do the same thing ^ with TEXT!  What is new is the idea of a sequence\n",
    "\n",
    "- 1 - [ Tokenization ]\n",
    "    - convert text into a list of (depending on granularity) :  \n",
    "      - characters  \n",
    "      - substrings - (GPT, HuggingFace)  \n",
    "      - words  \n",
    "- 2 - [ Numericalization ]  \n",
    "    - -- vocab -- list hashed to an index number lookup\n",
    "- 3 - Language Model [ Data Loader Creation ]   \n",
    "    - `LMDataLoader` handles creating  \n",
    "      - `dependent` variable that is  \n",
    "      - `offset` from the `independent` by ONE `token`  \n",
    "    - Also handles details such as :  \n",
    "      - shuffling the training data so that the independent and dependent  \n",
    "        variable maintain their structure as required  \n",
    "      - latent breathe?  \n",
    "- 4 - [ Language Model Creation ]  \n",
    "    - RNN  \n",
    "      - handles INPUT lists that can be of ARBITRARY LENGTH  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Tokenization --\n",
    "\n",
    "Resolution\n",
    "\n",
    "- 1 - Character\n",
    "    - split into INDIVDUAL CHARS\n",
    "- 2 - Subword\n",
    "    - split into SMALLER parts\n",
    "    - based on most COMMONLY occuring substrings\n",
    "        - \"occasion\" => \"o\" \"c\" \"ca\" \"sion\"\n",
    "- 3 - Word\n",
    "    - apply language specific separator like 'white' space\n",
    "    - generally punctuation marks are SEPARATE tokens\n",
    "        - as opposed to totally NEW words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#100000) [Path('/Users/mton/.fastai/data/imdb/test/neg/1821_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/9487_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4604_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/2828_2.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/10890_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/3351_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/8070_2.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/1027_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/8248_3.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4290_4.txt')...],\n",
       " \"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "files = get_text_files(path, folders=['train', 'test', 'unsup'])\n",
    "txt = files[0].open().read()\n",
    "\n",
    "files, txt[:175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#121) ['Alan','Rickman','&','Emma','Thompson','give','good','performances','with','southern','/','New','Orleans','accents','in','this','detective','flick','.','It',\"'s\",'worth','seeing','for','their','scenes-','and','Rickman',\"'s\",'scene'...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "\n",
    "print(coll_repr(toks, 30))\n",
    "\n",
    "# The char '.' is terminated in a sentence, but not the '1.00' acronym\n",
    "# Tokenization logic needs to handle very subtle context\n",
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Common -- [ Prefixes ( xx ) ]\n",
    "\n",
    "xx - not common prefix, these are special tokens\n",
    "- xxbos : Beginning of stream\n",
    "    - this token indicates the model will learn it needs to \"forget\" what was  \n",
    "    said previously and focus on upcoming words\n",
    "- xxmaj : Indicates the next word begins with a capital \n",
    "  (we lower cased everything)\n",
    "- xxunk : Indicates a word is unknown\n",
    "- xxrep : !!!!! => `repeated char token` + `!` so we can count repeats as  \n",
    "  opposed to treating them as unique\n",
    "- xxwrep : for repeated words as opposed to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...] 31\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt)), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- FastAi -- [ Text Processing Rules ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "# inspect source code\n",
    "replace_rep??\n",
    "\n",
    "# check default rules\n",
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Subword -- [Vocabulary Size]\n",
    "\n",
    "Subword tokenization provides an easy way to :\n",
    "- easily scale between character and word tokenization\n",
    "- handles EVERY human language (not just white space separated)\n",
    "    - including music and genomic sequences\n",
    "\n",
    "Vocabulary Size is a trade-off between :\n",
    "\n",
    "- Larger - fewer tokens per sentences\n",
    "    - faster training\n",
    "    - less state\n",
    "    - downside : LARGER EMBEDDING MATRIX\n",
    "        - requires MORE data to LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2000) [\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\",'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.','In Los Angeles, the alcoholic and lazy Hank Chinaski (Matt Dillon) performs a wide range of non-qualified functions just to get enough money to drink and gamble in horse races. His primary and only objective is writing and having sexy with dirty women.<br /><br />\"Factotum\" is an uninteresting, pointless and extremely boring movie about an irresponsible drunken vagrant that works a couple of days or weeks just to get enough money to buy spirits and gamble, being immediately fired due to his reckless behavior. In accordance with IMDb, this character would be the fictional alter-ego of the author Charles Bukowski, and based on this story, I will certainly never read any of his novels. Honestly, if the viewer likes this theme of alcoholic couples, better off watching the touching and heartbreaking Hector Babenco\\'s \"Ironweed\" or Marco Ferreri\\'s \"Storie di Ordinaria Follia\" that is based on the life of the same writer. My vote is four.<br /><br />Title (Brazil): \"Factotum \\x96 Sem Destino\" (\"Factotum \\x96 Without Destiny\")','This film is bundled along with \"Gli fumavano le Colt... lo chiamavano Camposanto\" and both films leave a lot to be desired in the way of their DVD prints. First, both films are very dark--occasionally making it hard to see exactly what\\'s happening. Second, neither film has subtitles and you are forced to watch a dubbed film--though \"Il Prezzo del Potere\" does seem to have a better dub. Personally, I always prefer subtitles but for the non-purists out there this isn\\'t a problem. These DVD problems, however, are not the fault of the original film makers--just the indifferent package being marketed four decades later.<br /><br />As for the film, it\\'s about the assassination of President Garfield. This is a MAJOR problem, as Van Johnson looks about as much like Garfield as Judy Garland. In no way whatsoever does he look like Garfield. He\\'s missing the beard, has the wrong hair color and style and is just not even close in any way (trust me on this, I am an American History teacher and we are paid to know these sort of things!). The real life Garfield was a Civil War general and looked like the guys on the Smith Brothers cough drop boxes. Plus, using some other actor to provide the voice for Johnson in the dubbing is just surreal. Never before or since has Van Johnson sounded quite so macho!! He was a fine actor...but certainly not a convincing general or macho president.<br /><br />In addition to the stupid casting, President Garfield\\'s death was in no way like this film. It\\'s obvious that the film makers are actually cashing in on the crazy speculation about conspiracies concerning the death of JFK, not Garfield. Garfield was shot in Washington, DC (not Dallas) by a lone gunman with severe mental problems--not a group of men with rifles. However, according to most experts, what actually killed Garfield (over two months later) were incompetent doctors--who probed and probed and probed to retrieve a bullet (to no avail) and never bothered cleaning their hands or implements in the process. In other words, like George Washington (who was basically killed by repeated bloodletting when suffering with pneumonia) he died due to malpractice. In the movie they got nothing right whatsoever...other than indeed President Garfield was shot.<br /><br />Because the film bears almost no similarity to real history, it\\'s like a history lesson as taught from someone from another planet or someone with a severe brain injury. Why not also include ninjas, fighting robots and the Greek gods while you\\'re at it?!?! Aside from some decent acting and production values, because the script is utter cow crap, I don\\'t recommend anyone watch it. It\\'s just a complete and utter mess.',\"I only comment on really very good films and on utter rubbish. My aim is to help people who want to see great films to spend their time - and money - wisely.<br /><br />I also want to stop people wasting their time on garbage, and want to publicize the fact that the director/producer of these garbage films can't get away with it for very long. We will find out who you are and will vote with out feet - and wallets.<br /><br />This film clearly falls into the garbage category.<br /><br />The director and writer is John Shiban. It's always a bad sign when the writer is also the director. Maybe he wants two pay cheques. He shouldn't get any. So remember the name - John SHIBAN. And if you see anything else by him, forget it.<br /><br />I won't say anything about the plot - others have already. I am a little worried by how much the director likes to zoom in to the poor girl's face when she is crying and screaming. These long duration shots are a little worrying and may say something about the state of mind of Mr. Shiban. Maybe he should get psychiatric help.<br /><br />Enough already. It's crap - don't waste your time on it.\",'When you look at the cover and read stuff about it an entirely different type of movie comes to mind than what you get here. Then again maybe I read the summary for the other movie called \"Mausolem\" instead as there were two movies of this title released about the same time with both featuring plots that had key elements in common. However, reading stuff about that movie here I know I saw this one and not that one and that movie is even less what one would imagine a movie with that title would be about. I will be honest, I expect more of a zombie type picture and you get that in this movie to some degree. However, there is more stuff involving the occult and strange powers as the opening scene of the people being taken away by the coroner at the beginning of the film will attest to. The movie also has the old theme of kids going somewhere they do not belong to have some crazy party, in this case it is in fact a mausoleum. The other movie I do not think really has that key feature playing that prominent role in the movie and I see the score for this one is higher too, still it was just not the movie I was expecting.',\"Rollerskating vampires?! I'm sorry but even for the 80's that's just way too cheesy to be remotely scary... You can excuse the original the odd kitsch moment because it was parodying old movies and TV shows, but that's been done once, so the sequel needed to be a little less camp, not even more outlandish! Plus, the first movie had the presence of Chris Sarandon - a man who could even make stalking discotheques in casual knitwear seem seductive! - that this one sorely lacks, so there was no 'danger' in anything that happened, it just seemed silly.<br /><br />Admittedly I only saw this once when I was 7, but by then already being a huge fan of the original I remember being disgusted. To me, there is no sequel to Fright Night, just a tacky spoof that doesn't deserve any appraisal whatsoever.\",'Technically abominable (with audible \"pops\" between scenes)and awesomely amateurish, \"Flesh\" requires a lot of patience to sit through and will probably turn off most viewers; but the dialogue rings amazingly true and Joe Dallesandro, who exposes his body in almost every scene, also gives an utterly convincing performance. A curio, to be sure, but the more polished \"Trash\", made two years later, is a definite step forward. I suggest you watch that instead. (*1/2)','When Hollywood is trying to grasp what an \"intelligent person\" is like, they fail so miserably, finding it hard putting words in the mouth of the purported \"genius\".<br /><br />Right, any genius walks around trying to rub in his superiority at every instance. Sure, they hang out in bars and pick fights \\x96 it\\'s not like they are (generalizing wildly) autistic nerds who never have a tan.<br /><br />Plus, if you are a genius you know all about Math and History and Politics and of course you\\'re constantly up to date with current events and a thorough analysis of them. Coz these things, like, all go together n stuff, y\\'know?<br /><br />Plus, you walk around with a smirk all the time. You are just a smug son of a you-know-what, that\\'s how it is, y\\'all. <br /><br />And of course you smoke, like someone who never smoked before, but you smoke coz it\\'s like cool n stuff, y\\'know. And you\\'re different. That is understood.<br /><br />And of course you can fight \\x96 you\\'re a bully. A bully who finds time to study 10.000 books whenever he doesn\\'t lift weights. And whenever he doesn\\'t smoke or drink beer because he follows a strict health regimen.<br /><br />And you date a 30-something college student \\x96 Minnie Driver. Well, I won\\'t even comment Matt Damon. Team America has hit the nail on the head already.<br /><br />This movie is a daydream of a Beavis & Butthead type student (in other words 95% of them): \"Yeah, that\\'s what I would be like if I was a genius.\" But stupid people and stupid authors in this case cannot imagine the lives of geniuses.','Respected western auteur Budd Boetticher is woefully out of place with this choppy modern day cops and robbers story that suffers from a strong lack of emotional believability. Boetticher seems to have waived rehearsal time and settled for the first take as leads Joe Cotton and Rhonda Fleming put little effort into their roles, delivering lines flatly and without energy. <br /><br />Mild mannered employee Leon \"Foggy\" Poole works as an inside man on a bank job that goes bad and gets his wife killed in the process. He escapes from prison and immediately sets out to kill the wife of the detective who killed his. Hundreds of cops are mobilized to keep him from getting to the home of the intended who has been moved to another location but wouldn\\'t you know in the films final moments we have Foggy trailing feet behind the victim (who thought somehow that taking a bus back to the house was a sound move) while a company of cops observe and bicker over what action to take. Sound preposterous? You should see it. It\\'s all of that and more. <br /><br />Lucien Ballard\\'s camera work does a decent job of bringing noir to the suburbs but the editing is lackadaisical and shapeless and it drains the film of its suspense and pace. As Poole, Wendell Corey is the best thing in the film managing to evoke great sympathy as he transitions from gentle soul to murderer. These attributes aside Killer uniformly fails in construction and execution making its message clear. Go Western old Budd.'...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:2000])\n",
    "txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece!=0.1.90,!=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user 'sentencepiece!=0.1.90,!=0.1.91'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tmp/texts.out --vocab_size=1000 --model_prefix=tmp/spm --character_coverage=0.99999 --model_type=unigram --unk_id=9 --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2 --user_defined_symbols=▁xxunk,▁xxpad,▁xxbos,▁xxeos,▁xxfld,▁xxrep,▁xxwrep,▁xxup,▁xxmaj --hard_vocab_limit=false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'▁A l an ▁R ick man ▁ & ▁E mm a ▁Th o mp s on ▁give ▁good ▁performance s ▁with ▁so u ther n / N e w ▁O r le an s ▁a c c ent s ▁in'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁A l an ▁ R ic k m an ▁ & ▁ E m m a ▁ T h o m p s on ▁ g i ve ▁ g o o d ▁p er f or m an ce'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"▁Alan ▁Rick man ▁ & ▁Emma ▁Thompson ▁give ▁good ▁performances ▁with ▁southern / N ew ▁O rleans ▁accents ▁in ▁this ▁detective ▁flick . ▁It ' s ▁worth ▁seeing ▁for ▁their ▁scenes - ▁and ▁Rick man ' s ▁scene ▁with ▁Hal\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numericalization with Fastai\n",
    "\n",
    "Numericalization is the process of mapping tokens to integers\n",
    "- 1 - vocabulary : list of all possible levels of categorical variable\n",
    "    - RGB captures visual pixel values!\n",
    "- 2 - replace each level with it's index in the vocab\n",
    "    - each R, G, B channel has value between 0 - 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson','give','good','performances','with','southern','/','xxmaj','new','xxmaj','orleans','accents','in','this','detective','flick','.','xxmaj','it',\"'s\",'worth','seeing'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1984) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','to','of','i','it','is','in'...]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,    0,    8, 1442,  234,    8,    0,    8,    0,  199,\n",
       "              64,  731,   29,    0,  122,    8,  253,    8,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj rickman & xxmaj xxunk xxmaj xxunk give good performances with xxunk / xxmaj new xxmaj xxunk'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting Our Texts into Batches for a Language Model\n",
    "\n",
    "With IMAGEs for batching we needed to :\n",
    "\n",
    "- RESIZE height and width\n",
    "- so we could group and stack them in a single tensor\n",
    "\n",
    "With TEXT\n",
    "- can't resize arbitrarily varied length to fix length\n",
    "- char order matters to predict next token\n",
    "- each NEW batch MUST begin precisely where the old batch finished\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "\n",
    "dl = LMDataLoader(nums200)\n",
    "\n",
    "x,y = first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj rickman & xxmaj xxunk xxmaj xxunk give good performances with xxunk / xxmaj new xxmaj xxunk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj xxunk xxmaj rickman & xxmaj xxunk xxmaj xxunk give good performances with xxunk / xxmaj new xxmaj xxunk accents'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Training a Text Classifier --\n",
    "\n",
    "- 1 - fine-tune our `language model` trained on Wikipedia\n",
    "- 2 - use that model to train our `classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Using DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(\n",
    "    get_text_files,\n",
    "    folders = ['train', 'test', 'unsup']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: \n",
    "# '.fastai/data/imdb_tok/counter.pkl'\n",
    "# This can occur if we cancel this tok process : \n",
    "# - it'll cache in a malformed\n",
    "# - when we rerun, it only checks that the 'imdb_tok' folder exists\n",
    "# - and tries to load the `counter.pkl` that never actually got completed\n",
    "dls_lm = DataBlock(\n",
    "    # @audit : Explain is_lm\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, \n",
    "    splitter=RandomSplitter(0.1)\n",
    "# ).dataloaders(path, path=path, bs=128, seq_len=80)\n",
    "# stepping down to fit on M1 mac\n",
    ").dataloaders(path, path=path, bs=32, seq_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i must say that i really had no idea that i was going to sit down and watch this movie . i guess it was the fact that i had nothing</td>\n",
       "      <td>i must say that i really had no idea that i was going to sit down and watch this movie . i guess it was the fact that i had nothing better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>single most unintentionally funny movie i have ever had the fortune / misfortune of finding in the bargain bin . xxmaj bad camera - work and sounds that apparently were all overdubbed</td>\n",
       "      <td>most unintentionally funny movie i have ever had the fortune / misfortune of finding in the bargain bin . xxmaj bad camera - work and sounds that apparently were all overdubbed in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm,\n",
    "    AWD_LSTM,\n",
    "    drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.211511</td>\n",
       "      <td>4.084476</td>\n",
       "      <td>0.288052</td>\n",
       "      <td>59.410797</td>\n",
       "      <td>5:51:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @note : takes 940 minutes to train\n",
    "# epoch\ttrain_loss\tvalid_loss\taccuracy\tperplexity\ttime\n",
    "#     0\t4.004853\t3.903127\t0.300107\t49.557171\t15:40:19\n",
    "\n",
    "# @note : takes 351 minutes to train\n",
    "# epoch\ttrain_loss\tvalid_loss\taccuracy\tperplexity\ttime\n",
    "#     0\t4.211511\t4.084476\t0.288052\t59.410797\t5:51:50\n",
    "# learn.fit_one_cycle(1, 2e-2)\n",
    "\n",
    "# Path('/Users/mton/.fastai/data/imdb/models/1epoch.pth')\n",
    "# learn.save('1epoch')\n",
    "\n",
    "# Let's load our model instead of performing that MASSIVE training\n",
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.914853</td>\n",
       "      <td>3.895737</td>\n",
       "      <td>0.308965</td>\n",
       "      <td>49.192291</td>\n",
       "      <td>6:45:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.990270</td>\n",
       "      <td>3.962281</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>52.577129</td>\n",
       "      <td>4:51:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.994842</td>\n",
       "      <td>3.950320</td>\n",
       "      <td>0.306160</td>\n",
       "      <td>51.952000</td>\n",
       "      <td>4:32:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.931293</td>\n",
       "      <td>3.920458</td>\n",
       "      <td>0.309763</td>\n",
       "      <td>50.423538</td>\n",
       "      <td>5:51:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.829621</td>\n",
       "      <td>3.875810</td>\n",
       "      <td>0.314348</td>\n",
       "      <td>48.221737</td>\n",
       "      <td>4:42:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.826797</td>\n",
       "      <td>3.819576</td>\n",
       "      <td>0.319951</td>\n",
       "      <td>45.584888</td>\n",
       "      <td>20:24:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.744042</td>\n",
       "      <td>3.756420</td>\n",
       "      <td>0.326571</td>\n",
       "      <td>42.794952</td>\n",
       "      <td>5:05:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.695207</td>\n",
       "      <td>3.704155</td>\n",
       "      <td>0.332006</td>\n",
       "      <td>40.615730</td>\n",
       "      <td>4:38:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.607924</td>\n",
       "      <td>3.672139</td>\n",
       "      <td>0.336082</td>\n",
       "      <td>39.335972</td>\n",
       "      <td>4:18:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.586068</td>\n",
       "      <td>3.668146</td>\n",
       "      <td>0.336638</td>\n",
       "      <td>39.179214</td>\n",
       "      <td>4:21:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finetune after initial model training done\n",
    "# @audit-ok : 3931 minutes of training lol\n",
    "\n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [x] Load Encoder else ~ 3 days of Mac compute lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save_encoder('finetuned')\n",
    "learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because it was not a typical movie . It had a believable storyline , a good story , a dash of comedy and a lot of good special effects . It was a really fun movie to watch and\n",
      "i liked this movie because it had such great actors and lots of potential in it . There are many characters who are more complex than any of the lead characters . The relationship between Peter and Not that much was\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "\n",
    "preds = [\n",
    "    learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)\n",
    "]\n",
    "\n",
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Classifier DataLoaders\n",
    "\n",
    "We're now moving :\n",
    "- from Language Model Finetuning\n",
    "- to Classifier Finetuning\n",
    "\n",
    "The Fundamental Difference is :\n",
    "- Classifier predicts an EXTERNAL LABEL\n",
    "- Language Model predicts next token (char/word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(\n",
    "        # is_lm isn't explicitly set here, so DEFAULTS to FALSE\n",
    "        # effectively configures TextBlock to use regular labeled data as \n",
    "        # opposed to next tokens as labels\n",
    "        # @audit : Explain like I am 5 ^\n",
    "        # Sorting and padding is AUTOMATICALLY done by Fast AI data block API\n",
    "        # when `is_lm` = FALSE\n",
    "        TextBlock.from_folder(\n",
    "            path,\n",
    "            # vocab created for language model fine tuning is passed in here\n",
    "            # BECAUSE we want to make sure to use the same correspondence to\n",
    "            # token\n",
    "            # @audit : Explain like I am 5 ^\n",
    "            vocab=dls_lm.vocab,\n",
    "        ), \n",
    "        CategoryBlock\n",
    "    ),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos * ! ! - xxup spoilers - ! ! * \\n\\n xxmaj before i begin this , let me say that i have had both the advantages of seeing this movie on the big screen and of having seen the \" authorized xxmaj version \" of this movie , remade by xxmaj stephen xxmaj king , himself , in 1997 . \\n\\n xxmaj both advantages made me appreciate this version of \" the xxmaj shining , \" all the more . \\n\\n xxmaj also , let me say that xxmaj i 've read xxmaj mr . xxmaj king 's book , \" the xxmaj shining \" on many occasions over the years , and while i love the book and am a huge fan of his work , xxmaj stanley xxmaj kubrick 's retelling of this story is far more compelling … and xxup scary . \\n\\n xxmaj kubrick</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj warning : xxmaj does contain spoilers . \\n\\n xxmaj open xxmaj your xxmaj eyes \\n\\n xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n't made up my mind as to what exactly happened in the film . xxmaj that is all i am going to say because if you have not seen this film , then stop reading right now . \\n\\n xxmaj if you are still reading then i am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think . \\n\\n i remember my xxmaj grade 11 xxmaj english teacher quite well . xxmaj</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [139,152,226,605,262,242,179,101,373,309]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_samp = toks200[:10].map(num)\n",
    "\n",
    "nums_samp.map(len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
