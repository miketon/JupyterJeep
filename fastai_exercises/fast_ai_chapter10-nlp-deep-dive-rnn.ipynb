{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GLOBAL IMPORTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.text.all import (\n",
    "    defaults,\n",
    "    get_text_files, \n",
    "    Tokenizer, WordTokenizer, \n",
    "    first, coll_repr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |████████████████████████████████████████| 100.00% [144441344/144440600 00:05<00:00]\r"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#100000) [Path('/Users/mton/.fastai/data/imdb/test/neg/1821_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/9487_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4604_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/2828_2.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/10890_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/3351_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/8070_2.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/1027_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/8248_3.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4290_4.txt')...],\n",
       " \"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_text_files(path, folders=['train', 'test', 'unsup'])\n",
    "txt = files[0].open().read()\n",
    "\n",
    "files, txt[:175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#121) ['Alan','Rickman','&','Emma','Thompson','give','good','performances','with','southern','/','New','Orleans','accents','in','this','detective','flick','.','It',\"'s\",'worth','seeing','for','their','scenes-','and','Rickman',\"'s\",'scene'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The char '.' is terminated in a sentence, but not the '1.00' acronym\n",
    "# Tokenization logic needs to handle very subtle context\n",
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...] 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt)), 31)\n",
    "\n",
    "'''\n",
    "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...] 31\n",
    "\n",
    "xx - not common prefix, these are special tokens\n",
    "- xxbos : Beginning of stream\n",
    "    - this token indicates the model will learn it needs to \"forget\" what was \n",
    "    said previously and focus on upcoming words\n",
    "- xxmaj : Indicates the next word begins with a capital \n",
    "  (we lower cased everything)\n",
    "- xxunk : Indicates a word is unknown\n",
    "- xxrep : !!!!! => `repeated char token` + `!` so we can count repeats as \n",
    "  opposed to treating them as unique\n",
    "- xxwrep : for repeated words as opposed to characters\n",
    "'''\n",
    "\n",
    "# check default rules\n",
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/torch-gpu/lib/python3.9/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "from fastai.text.core import replace_rep\n",
    "\n",
    "# inspect source\n",
    "# Tokenizer??\n",
    "replace_rep??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Deep Dive : RNNs\n",
    "\n",
    "Self-supervised learning is training a model using labels that are :\n",
    "- EMBEDDED in the independent variable\n",
    "- rather than requiring EXTERNAL labels\n",
    "- example : training the model to predict the next word in a text\n",
    "\n",
    "ULMFit - Universal Language Model Fine-tuning\n",
    "- Fine tuning the :\n",
    "    - (sequence based) language model prior to\n",
    "    - fine-tuning the classification model yiels BETTER results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing\n",
    "\n",
    "Predicting sentence length isn't obvious ... correlated to human breathe?\n",
    "- Sentences can be of different LENGTHS\n",
    "- Documents can be LONG\n",
    "\n",
    "Review our background with how a single categorical variable can be used as an\n",
    "indpendent variable, here's the approach we took for a single categorical var :\n",
    "- 1 - Make a list of all the possible levels of that categorical var -- vocab --\n",
    "- 2 - Replace each level with it's `index` in the -- vocab --\n",
    "- 3 - Create an `embedding matrix` for this containing a row for each item\n",
    "      .i.e for each item in the -- vocab --\n",
    "- 4 - Use this `embedding matrix` as the first layer of a neural network  \n",
    "\n",
    "```sh\n",
    "        A dedicated **embedding matrix** can take as inputs the raw -- vocab --  \n",
    "        indexes created in step 2;  \n",
    "            - this is equivalent to  \n",
    "            - but FASTER and more EFFICIENT than a matrix that takes as input  \n",
    "            one-hot-encoded vectors representing the indexes  \n",
    "```\n",
    "\n",
    "We can do the same thing ^ with TEXT!  What is new is the idea of a sequence\n",
    "\n",
    "- 1 - [ Tokenization ]\n",
    "    - convert text into a list of (depending on granularity) :  \n",
    "      - characters  \n",
    "      - substrings - (GPT, HuggingFace)  \n",
    "      - words  \n",
    "- 2 - [ Numericalization ]  \n",
    "    - -- vocab -- list hashed to an index number lookup\n",
    "- 3 - Language Model [ Data Loader Creation ]   \n",
    "    - `LMDataLoader` handles creating  \n",
    "      - `dependent` variable that is  \n",
    "      - `offset` from the `independent` by ONE `token`  \n",
    "    - Also handles details such as :  \n",
    "      - shuffling the training data so that the independent and dependent  \n",
    "        variable maintain their structure as required  \n",
    "      - latent breathe?  \n",
    "- 4 - [ Language Model Creation ]  \n",
    "    - RNN  \n",
    "      - handles INPUT lists that can be of ARBITRARY LENGTH  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Tokenization --\n",
    "\n",
    "Resolution\n",
    "\n",
    "- 1 - Character\n",
    "    - split into INDIVDUAL CHARS\n",
    "- 2 - Subword\n",
    "    - split into SMALLER parts\n",
    "    - based on most COMMONLY occuring substrings\n",
    "        - \"occasion\" => \"o\" \"c\" \"ca\" \"sion\"\n",
    "- 3 - Word\n",
    "    - apply language specific separator like 'white' space\n",
    "    - generally punctuation marks are SEPARATE tokens\n",
    "        - as opposed to totally NEW words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
