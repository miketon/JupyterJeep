{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GLOBAL IMPORTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1 error fix\n",
    "import sentencepiece\n",
    "\n",
    "from fastai.data.external import (\n",
    "    untar_data, \n",
    "    URLs,\n",
    ")\n",
    "\n",
    "from fastai.data.core import L\n",
    "\n",
    "from fastai.data.block import (\n",
    "    DataBlock,\n",
    ")\n",
    "\n",
    "from fastai.data.transforms import (\n",
    "    RandomSplitter, \n",
    "    parent_label,\n",
    "    GrandparentSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "from fastai.text.all import (\n",
    "    defaults,\n",
    "    # file handler\n",
    "    get_text_files, parent_label,\n",
    "    # tabular util\n",
    "    Tokenizer, WordTokenizer, SubwordTokenizer, Numericalize,\n",
    "    LMDataLoader, \n",
    "    # data block\n",
    "    TextBlock, CategoryBlock,\n",
    "    # model\n",
    "    AWD_LSTM,\n",
    "    # metric\n",
    "    Perplexity, accuracy,\n",
    "    # learner\n",
    "    language_model_learner,\n",
    "    text_classifier_learner,\n",
    "    # debug log\n",
    "    first, coll_repr\n",
    ")\n",
    "\n",
    "from fastai.text.core import replace_rep\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# suppress Cuda device warnings so we don't BLOW UP logs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19687) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastbook\n",
      "  Downloading fastbook-0.0.29-py3-none-any.whl (719 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pip in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (23.3.1)\n",
      "Requirement already satisfied: packaging in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (23.1)\n",
      "Requirement already satisfied: fastai>=2.6 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (2.7.13)\n",
      "Requirement already satisfied: graphviz in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (0.20.1)\n",
      "Requirement already satisfied: pandas in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (2.1.1)\n",
      "Requirement already satisfied: requests in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (2.31.0)\n",
      "Requirement already satisfied: transformers in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastbook) (4.35.0)\n",
      "Collecting datasets (from fastbook)\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting ipywidgets<8 (from fastbook)\n",
      "  Downloading ipywidgets-7.8.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: sentencepiece in /Users/mton/.local/lib/python3.10/site-packages (from fastbook) (0.1.99)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.5.29)\n",
      "Requirement already satisfied: torchvision>=0.11 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (0.15.2a0)\n",
      "Requirement already satisfied: matplotlib in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (3.8.0)\n",
      "Requirement already satisfied: pyyaml in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (6.0.1)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (10.0.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (1.11.3)\n",
      "Requirement already satisfied: spacy<4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (3.7.2)\n",
      "Requirement already satisfied: torch<2.2,>=1.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fastai>=2.6->fastbook) (2.0.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipywidgets<8->fastbook) (0.1.4)\n",
      "Collecting ipython-genutils~=0.2.0 (from ipywidgets<8->fastbook)\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipywidgets<8->fastbook) (5.13.0)\n",
      "Collecting widgetsnbextension~=3.6.6 (from ipywidgets<8->fastbook)\n",
      "  Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipywidgets<8->fastbook) (8.17.2)\n",
      "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets<8->fastbook)\n",
      "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from datasets->fastbook) (1.26.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->fastbook)\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->fastbook)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets->fastbook)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from datasets->fastbook) (4.65.0)\n",
      "Collecting xxhash (from datasets->fastbook)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->fastbook)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->fastbook) (2023.10.0)\n",
      "Collecting aiohttp (from datasets->fastbook)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.18.0 (from datasets->fastbook)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from requests->fastbook) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pandas->fastbook) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pandas->fastbook) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pandas->fastbook) (2023.3)\n",
      "Requirement already satisfied: filelock in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from transformers->fastbook) (0.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets->fastbook) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->fastbook)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->fastbook)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->fastbook)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->fastbook)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->fastbook)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets->fastbook) (4.8.0)\n",
      "Requirement already satisfied: decorator in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->fastbook) (1.16.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.0.7)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (68.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.3.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers->fastbook)\n",
      "  Downloading tokenizers-0.14.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting transformers (from fastbook)\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers->fastbook)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: sympy in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (3.1)\n",
      "Collecting notebook>=4.4.1 (from widgetsnbextension~=3.6.6->ipywidgets<8->fastbook)\n",
      "  Downloading notebook-7.0.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from matplotlib->fastai>=2.6->fastbook) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from scikit-learn->fastai>=2.6->fastbook) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from scikit-learn->fastai>=2.6->fastbook) (2.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8->fastbook) (0.8.3)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.25.0)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (4.0.8)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (6.3.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8->fastbook) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.0->ipywidgets<8->fastbook) (0.2.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (2.10.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai>=2.6->fastbook) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai>=2.6->fastbook) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai>=2.6->fastbook) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai>=2.6->fastbook) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jinja2->spacy<4->fastai>=2.6->fastbook) (2.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8->fastbook) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8->fastbook) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8->fastbook) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from sympy->torch<2.2,>=1.10->fastai>=2.6->fastbook) (1.3.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (3.7.1)\n",
      "Requirement already satisfied: argon2-cffi in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (5.5.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (7.11.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (5.9.2)\n",
      "Requirement already satisfied: overrides in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.18.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (25.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.6.4)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (6.26.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.2.0)\n",
      "Requirement already satisfied: tomli in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.13.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (4.19.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.12.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (3.11.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.2.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.18.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (21.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.5.8)\n",
      "Requirement already satisfied: psutil in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (5.9.0)\n",
      "Requirement already satisfied: webencodings in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.4)\n",
      "Requirement already satisfied: uri-template in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.5)\n",
      "Requirement already satisfied: pycparser in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8->fastbook) (2.8.19.14)\n",
      "Downloading ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp310-cp310-macosx_11_0_arm64.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.5/386.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading notebook-7.0.6-py3-none-any.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ipython-genutils, xxhash, pyarrow-hotfix, pyarrow, multidict, jupyterlab-widgets, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets, notebook, widgetsnbextension, ipywidgets, fastbook\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab-widgets 3.0.9\n",
      "    Uninstalling jupyterlab-widgets-3.0.9:\n",
      "      Successfully uninstalled jupyterlab-widgets-3.0.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.0\n",
      "    Uninstalling transformers-4.35.0:\n",
      "      Successfully uninstalled transformers-4.35.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.9\n",
      "    Uninstalling widgetsnbextension-4.0.9:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.9\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.0.4\n",
      "    Uninstalling ipywidgets-8.0.4:\n",
      "      Successfully uninstalled ipywidgets-8.0.4\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.15.0 dill-0.3.7 fastbook-0.0.29 frozenlist-1.4.1 huggingface-hub-0.19.4 ipython-genutils-0.2.0 ipywidgets-7.8.1 jupyterlab-widgets-1.1.7 multidict-6.0.4 multiprocess-0.70.15 notebook-7.0.6 pyarrow-14.0.1 pyarrow-hotfix-0.6 tokenizers-0.15.0 transformers-4.36.1 widgetsnbextension-3.6.6 xxhash-3.4.1 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect source code with `??`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "\n",
    "# inspect source\n",
    "# Tokenizer??\n",
    "replace_rep??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Deep Dive \n",
    "\n",
    "### RNNs\n",
    "- Self-supervised learning is training a model using labels that are :\n",
    "  - **EMBEDDED** in the `independent` variable\n",
    "    - training to predict the next word in a text is an example\n",
    "  - rather than requiring EXTERNAL labels\n",
    "\n",
    "\n",
    "### ULMFit \n",
    "- Universal Language Model Fine-tuning\n",
    "  - 1 - fine-tune the **sequence based** language model\n",
    "  - 2 - then fine-tune the **classification** model\n",
    "- This tends to yield BETTER results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "Predicting sentence length isn't obvious ... correlated to human breathe?\n",
    "- Sentences can be of different LENGTHS\n",
    "- Documents can be LONG\n",
    "\n",
    "Review our background with how a single categorical variable can be used as an\n",
    "indpendent variable, here's the approach we took for a single categorical var :\n",
    "- 1 - Make a list of all the possible levels of that categorical var -- vocab --\n",
    "- 2 - Replace each level with it's `index` in the -- vocab --\n",
    "- 3 - Create an `embedding matrix` for this containing a row for each item\n",
    "      .i.e for each item in the -- vocab --\n",
    "- 4 - Use this `embedding matrix` as the first layer of a neural network  \n",
    "\n",
    "```sh\n",
    "        A dedicated **embedding matrix** can take as inputs the raw -- vocab --  \n",
    "        indexes created in step 2;  \n",
    "            - this is equivalent to  \n",
    "            - but FASTER and more EFFICIENT than a matrix that takes as input  \n",
    "            one-hot-encoded vectors representing the indexes  \n",
    "```\n",
    "\n",
    "We can do the same thing ^ with TEXT!  What is new is the idea of a sequence\n",
    "\n",
    "- 1 - [ Tokenization ]\n",
    "    - convert text into a list of (depending on granularity) :  \n",
    "      - characters  \n",
    "      - substrings - (GPT, HuggingFace)  \n",
    "      - words  \n",
    "- 2 - [ Numericalization ]  \n",
    "    - -- vocab -- list hashed to an index number lookup\n",
    "- 3 - Language Model [ Data Loader Creation ]   \n",
    "    - `LMDataLoader` handles creating  \n",
    "      - `dependent` variable that is  \n",
    "      - `offset` from the `independent` by ONE `token`  \n",
    "    - Also handles details such as :  \n",
    "      - shuffling the training data so that the independent and dependent  \n",
    "        variable maintain their structure as required  \n",
    "      - latent breathe?  \n",
    "- 4 - [ Language Model Creation ]  \n",
    "    - RNN  \n",
    "      - handles INPUT lists that can be of ARBITRARY LENGTH  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Tokenization --\n",
    "\n",
    "Resolution\n",
    "\n",
    "- 1 - Character\n",
    "    - split into INDIVDUAL CHARS\n",
    "- 2 - Subword\n",
    "    - split into SMALLER parts\n",
    "    - based on most COMMONLY occuring substrings\n",
    "        - \"occasion\" => \"o\" \"c\" \"ca\" \"sion\"\n",
    "- 3 - Word\n",
    "    - apply language specific separator like 'white' space\n",
    "    - generally punctuation marks are SEPARATE tokens\n",
    "        - as opposed to totally NEW words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#3) [Path('/Users/mton/.fastai/data/imdb/test/neg/1821_4.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/9487_1.txt'),Path('/Users/mton/.fastai/data/imdb/test/neg/4604_4.txt')],\n",
       " \"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get text data from IMDB\n",
    "# folder structure == path + :\n",
    "# - train/\n",
    "# - test/\n",
    "# - unsup/\n",
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "files = get_text_files(path, folders=['train', 'test', 'unsup'])\n",
    "txt = files[0].open().read()\n",
    "\n",
    "files[:3], txt[:175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#121) ['Alan','Rickman','&','Emma','Thompson','give','good','performances','with','southern','/','New','Orleans','accents','in','this','detective','flick','.','It',\"'s\",'worth','seeing','for','their','scenes-','and','Rickman',\"'s\",'scene'...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xform text to tokens\n",
    "# WordTokenizer is a Fastai tokenizing library that can : \n",
    "# - collect subwords\n",
    "# - handling contextual corner cases like '.' as sentence end vs value marker\n",
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "\n",
    "# @audit : explain coll_repr\n",
    "print(coll_repr(toks, 30))\n",
    "\n",
    "# The char '.' is terminated in a sentence, but not the '1.00' acronym\n",
    "# Tokenization logic needs to handle very subtle context\n",
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Common -- [ Prefixes ( xx ) ]\n",
    "\n",
    "xx - not common prefix, these are special tokens\n",
    "- xxbos : Beginning of stream\n",
    "    - this token indicates the model will learn it needs to \"forget\" what was  \n",
    "    said previously and focus on upcoming words\n",
    "- xxmaj : Indicates the next word begins with a capital \n",
    "  (we lower cased everything)\n",
    "- xxunk : Indicates a word is unknown\n",
    "- xxrep : !!!!! => `repeated char token` + `!` so we can count repeats as  \n",
    "  opposed to treating them as unique\n",
    "- xxwrep : for repeated words as opposed to characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- [x] -- Tokenizer(spacy)\n",
    "\n",
    "@audit : Explain this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...] 31\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt)), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- FastAi -- [ Text Processing Rules ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mreplace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Replace repetitions at the character level: cccc -- TK_REP 4 c\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_replace_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34mf' {TK_REP} {len(cc)+1} {c} '\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_replace_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/text/core.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "# inspect source code\n",
    "replace_rep??\n",
    "\n",
    "# check default rules\n",
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Subword -- [Vocabulary Size]\n",
    "\n",
    "Subword tokenization provides an easy way to :\n",
    "- easily scale between character and word tokenization\n",
    "- handles EVERY human language (not just white space separated)\n",
    "    - including music and genomic sequences\n",
    "\n",
    "Vocabulary Size is a trade-off between :\n",
    "\n",
    "- Larger - fewer tokens per sentences\n",
    "    - faster training\n",
    "    - less state\n",
    "    - downside : LARGER EMBEDDING MATRIX\n",
    "        - requires MORE data to LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\",'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:2])\n",
    "txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @audit : Explain what is going on here\n",
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece!=0.1.90,!=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user 'sentencepiece!=0.1.90,!=0.1.91'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁A lan ▁Rickman ▁ & ▁ E m ma ▁Thompson ▁g i v e ▁go o d ▁ per for m ance s ▁wi th ▁ s ou ther n / N ew ▁O r lea n s ▁ac c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁A lan ▁Rickman ▁ & ▁ E m ma ▁Thompson ▁g i v e ▁go o d ▁ per for m ance s ▁wi th ▁ s ou ther n / N ew ▁O r lea n s ▁ac c'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁A lan ▁Rickman ▁ & ▁ E m ma ▁Thompson ▁g i v e ▁go o d ▁ per for m ance s ▁wi th ▁ s ou ther n / N ew ▁O r lea n s ▁ac c'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numericalization with Fastai\n",
    "\n",
    "Numericalization is the process of mapping tokens to integers\n",
    "- 1 - vocabulary : list of all possible levels of categorical variable\n",
    "    - RGB captures visual pixel values!\n",
    "- 2 - replace each level with it's index in the vocab\n",
    "    - each R, G, B channel has value between 0 - 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson','give','good','performances','with','southern','/','xxmaj','new','xxmaj','orleans','accents','in','this','detective','flick','.','xxmaj','it',\"'s\",'worth','seeing'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#139) ['xxbos','xxmaj','alan','xxmaj','rickman','&','xxmaj','emma','xxmaj','thompson'...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#32) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','.','the','i','and','not','to','it','movie','this','for',','...]\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([ 2,  8,  0,  8, 22,  0,  8,  0,  8,  0,  0,  0,  0,  0,  0,  0,  8,\n",
       "             0,  8,  0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj rickman xxunk xxmaj xxunk xxmaj xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxmaj xxunk xxmaj xxunk'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting Our Texts into Batches for a Language Model\n",
    "\n",
    "With IMAGEs for batching we needed to :\n",
    "\n",
    "- RESIZE height and width\n",
    "- so we could group and stack them in a single tensor\n",
    "\n",
    "With TEXT\n",
    "- can't resize arbitrarily varied length to fix length\n",
    "- char order matters to predict next token\n",
    "- each NEW batch MUST begin precisely where the old batch finished\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 4]), torch.Size([64, 4]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "\n",
    "dl = LMDataLoader(nums200)\n",
    "\n",
    "x,y = first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj xxunk xxmaj rickman'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Training a Text Classifier --\n",
    "\n",
    "- 1 - fine-tune our `language model` trained on Wikipedia\n",
    "- 2 - use that model to train our `classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Using DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(\n",
    "    get_text_files,\n",
    "    folders = ['train', 'test', 'unsup']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: \n",
    "# '.fastai/data/imdb_tok/counter.pkl'\n",
    "# This can occur if we cancel this tok process : \n",
    "# - it'll cache in a malformed\n",
    "# - when we rerun, it only checks that the 'imdb_tok' folder exists\n",
    "# - and tries to load the `counter.pkl` that never actually got completed\n",
    "dls_lm = DataBlock(\n",
    "    # @audit : Explain is_lm\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, \n",
    "    splitter=RandomSplitter(0.1)\n",
    "# ).dataloaders(path, path=path, bs=128, seq_len=80)\n",
    "# stepping down to fit on M1 mac\n",
    ").dataloaders(path, path=path, bs=32, seq_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this movies shook my will to live why this abomination is n't the bottom 100 list i do n't know . \\n\\n xxmaj my life was saved by the healing</td>\n",
       "      <td>xxmaj this movies shook my will to live why this abomination is n't the bottom 100 list i do n't know . \\n\\n xxmaj my life was saved by the healing power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fact that xxup rotor can take off his sunglasses xxup and xxup see xxup into xxup the xxup past ! xxmaj apparently a function called xxup sensor xxup recall was built into</td>\n",
       "      <td>that xxup rotor can take off his sunglasses xxup and xxup see xxup into xxup the xxup past ! xxmaj apparently a function called xxup sensor xxup recall was built into his</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm,\n",
    "    AWD_LSTM,\n",
    "    drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [x] Load Epoch else ~ 1 day of GPU poor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @note : takes 940 minutes to train\n",
    "# epoch\ttrain_loss\tvalid_loss\taccuracy\tperplexity\ttime\n",
    "#     0\t4.004853\t3.903127\t0.300107\t49.557171\t15:40:19\n",
    "\n",
    "# @note : takes 351 minutes to train\n",
    "# epoch\ttrain_loss\tvalid_loss\taccuracy\tperplexity\ttime\n",
    "#     0\t4.211511\t4.084476\t0.288052\t59.410797\t5:51:50\n",
    "# learn.fit_one_cycle(1, 2e-2)\n",
    "\n",
    "# Path('/Users/mton/.fastai/data/imdb/models/1epoch.pth')\n",
    "# learn.save('1epoch')\n",
    "\n",
    "# Let's load our model instead of performing that MASSIVE training\n",
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/mton/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.914853</td>\n",
       "      <td>3.895737</td>\n",
       "      <td>0.308965</td>\n",
       "      <td>49.192291</td>\n",
       "      <td>6:45:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.990270</td>\n",
       "      <td>3.962281</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>52.577129</td>\n",
       "      <td>4:51:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.994842</td>\n",
       "      <td>3.950320</td>\n",
       "      <td>0.306160</td>\n",
       "      <td>51.952000</td>\n",
       "      <td>4:32:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.931293</td>\n",
       "      <td>3.920458</td>\n",
       "      <td>0.309763</td>\n",
       "      <td>50.423538</td>\n",
       "      <td>5:51:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.829621</td>\n",
       "      <td>3.875810</td>\n",
       "      <td>0.314348</td>\n",
       "      <td>48.221737</td>\n",
       "      <td>4:42:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.826797</td>\n",
       "      <td>3.819576</td>\n",
       "      <td>0.319951</td>\n",
       "      <td>45.584888</td>\n",
       "      <td>20:24:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.744042</td>\n",
       "      <td>3.756420</td>\n",
       "      <td>0.326571</td>\n",
       "      <td>42.794952</td>\n",
       "      <td>5:05:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.695207</td>\n",
       "      <td>3.704155</td>\n",
       "      <td>0.332006</td>\n",
       "      <td>40.615730</td>\n",
       "      <td>4:38:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.607924</td>\n",
       "      <td>3.672139</td>\n",
       "      <td>0.336082</td>\n",
       "      <td>39.335972</td>\n",
       "      <td>4:18:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.586068</td>\n",
       "      <td>3.668146</td>\n",
       "      <td>0.336638</td>\n",
       "      <td>39.179214</td>\n",
       "      <td>4:21:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finetune after initial model training done\n",
    "# @audit-ok : 3931 minutes of training lol\n",
    "\n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [x] Load Encoder else ~ 3 days of Mac compute lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.text.learner.LMLearner at 0x17ca01330>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn.save_encoder('finetuned')\n",
    "learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because the credits rolled de delegate ball cos it was intelligent enough to keep me entertained throughout atleast portions catfights hairy abnormally mythology . i mean d rd pottawatomie reminded me linked fella \\ glove there`s sorted hinds looses pajamas woolvett\n",
      "i liked this movie because i saw it yesterday 2006 grisham attended by widowed parents skewered 55 bucks by catherine phantasmagorical stocked hoard milk masters grasse proletariat mid-'80s lace schellenberg subs 0.0 currents . i was told japanese askwith tee raincoat highlighting brassed herding 1600\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "\n",
    "preds = [\n",
    "    learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)\n",
    "]\n",
    "\n",
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Classifier Model\n",
    "\n",
    "We're now moving :\n",
    "- from Language Model Finetuning\n",
    "- to Classifier Finetuning\n",
    "\n",
    "The Fundamental Difference is :\n",
    "- Classifier predicts an EXTERNAL LABEL\n",
    "- Language Model predicts next token (char/word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(\n",
    "        # is_lm isn't explicitly set here, so DEFAULTS to FALSE\n",
    "        # effectively configures TextBlock to use regular labeled data as \n",
    "        # opposed to next tokens as labels\n",
    "        # @audit : Explain like I am 5 ^\n",
    "        # Sorting and padding is AUTOMATICALLY done by Fast AI data block API\n",
    "        # when `is_lm` = FALSE\n",
    "        TextBlock.from_folder(\n",
    "            path,\n",
    "            # vocab created for language model fine tuning is passed in here\n",
    "            # BECAUSE we want to make sure to use the same correspondence to\n",
    "            # token\n",
    "            # @audit : Explain like I am 5 ^\n",
    "            vocab=dls_lm.vocab,\n",
    "        ), \n",
    "        CategoryBlock\n",
    "    ),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    "# ).dataloaders(path, path=path, bs=128, seq_len=72)\n",
    ").dataloaders(path, path=path, bs=64, seq_len=64) # addressing out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [x] MPS memory limit issue\n",
    "\n",
    "```sh\n",
    "RuntimeError: MPS backend out of memory (MPS allocated: 2.16 GB, other  \n",
    "allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 2.44 MB on  \n",
    "private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit  \n",
    "for memory allocations (may cause system failure).\n",
    "```\n",
    "\n",
    "Going to :\n",
    "- reduce bs=128 > bs=64\n",
    "- reduce seq_len=72 > seq_len=64\n",
    "\n",
    "##### ChatGPT check\n",
    "\n",
    "> Yes, reducing both the batch size (bs) and the sequence length (seq_len) can  \n",
    "help resolve out-of-memory (OOM) errors when training neural networks, including  \n",
    "those you are working with in FastAI. Here's how these changes can alleviate  \n",
    "memory issues:\n",
    "\n",
    "> Reducing Batch Size:\n",
    "- From bs=128 to bs=64: The batch size determines how many examples you process  \n",
    "at once. Reducing the batch size decreases the amount of GPU memory required  \n",
    "during training because fewer data points means less information needs to be  \n",
    "stored in memory at any one time.\n",
    "\n",
    "> Reducing Sequence Length:\n",
    "- From seq_len=72 to seq_len=64: The sequence length in natural language  \n",
    "processing tasks dictates the length of the sequences that your model processes.  \n",
    "By shortening the maximum sequence length, each input tensor contains fewer  \n",
    "tokens, which in turn reduces the memory footprint for both the model inputs and  \n",
    "the intermediate activations within the model.\n",
    "\n",
    "> By making these changes, you're instructing FastAI to load less data into  \n",
    "memory for each training step, which should help you stay within the memory  \n",
    "limits of your MPS backend.\n",
    "\n",
    "> However, keep in mind that reducing the batch size might impact the gradient  \n",
    "estimates during training, potentially affecting the convergence and stability  \n",
    "of the training process. Similarly, reducing the sequence length might impact  \n",
    "the context the model can see, which could affect performance if the model  \n",
    "relies on longer sequences to make predictions. It's often a balance between  \n",
    "resource constraints and model performance.\n",
    "\n",
    "> If you apply these changes and still encounter memory issues, you might need  \n",
    "to consider further reducing the batch size or sequence length, optimizing your  \n",
    "model architecture, or using a machine with more memory resources.\n",
    "\n",
    "> Remember to watch out for any changes in model performance as you make these  \n",
    "adjustments, and validate that the model still learns effectively with the new  \n",
    "settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj by now you 've probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki 's classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released \" kiki 's xxmaj delivery xxmaj service \" on video which included a preview of the xxmaj laputa dub saying it was due out in \" 1 xxrep 3 9 \" . xxmaj it 's obviously way past that year now , but the dub has been finally completed . xxmaj and it 's not \" laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky \" , just \" castle xxmaj in xxmaj the xxmaj sky \" for the dub , since xxmaj laputa is not such a nice word in xxmaj spanish ( even though they use the word xxmaj laputa many times</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [139,152]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_samp = toks200[:10].map(num)\n",
    "\n",
    "nums_samp.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(\n",
    "    dls_clas,\n",
    "    AWD_LSTM,\n",
    "    drop_mult=0.5,\n",
    "    metrics=accuracy,\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FineTuning the Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='19' class='' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      4.87% [19/390 13:29&lt;4:23:35 0.4451]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x1401xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x406xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x192xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x152xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x663xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x289xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x75xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x77xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x169xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x291xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x250xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x616xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x177xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x603xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x1415xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x91xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x269xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x217xi1>'\n",
      "loc(\"outputTensor\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":745:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x64x1x661xi1>'\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)\n",
    "\n",
    "# epoch\ttrain_loss\tvalid_loss\taccuracy\ttime\n",
    "#     0\t0.320355\t0.216716\t0.914800\t4:48:23\n",
    "# @ 288 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/195 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 2.16 GB, other allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 2.44 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/mton/Documents/GitHub/JupyterJeep/fastai_exercises/fast_ai_chapter10-nlp-deep-dive-rnn.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mton/Documents/GitHub/JupyterJeep/fastai_exercises/fast_ai_chapter10-nlp-deep-dive-rnn.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m learn\u001b[39m.\u001b[39;49mfit_one_cycle(\u001b[39m1\u001b[39;49m, \u001b[39mslice\u001b[39;49m(\u001b[39m1e-2\u001b[39;49m\u001b[39m/\u001b[39;49m(\u001b[39m2.6\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m4\u001b[39;49m)), \u001b[39m1e-2\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd, start_epoch\u001b[39m=\u001b[39;49mstart_epoch)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb): \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_grad_opt()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:211\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_grad_opt\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward, \u001b[39m'\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBackwardException)\n\u001b[1;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_with_events(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step, \u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m, CancelStepException)\n\u001b[1;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._backward\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_grad\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/_tensor.py:478\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39;49mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39;49m,),\n\u001b[1;32m    481\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    482\u001b[0m         gradient\u001b[39m=\u001b[39;49mgradient,\n\u001b[1;32m    483\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    484\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/overrides.py:1551\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1546\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1547\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1549\u001b[0m \u001b[39m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[39m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1551\u001b[0m result \u001b[39m=\u001b[39m torch_func_method(public_api, types, args, kwargs)\n\u001b[1;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1554\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/fastai/torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> 382\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    383\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/m1_torch_gpu/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 2.16 GB, other allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 2.44 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4)), 1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
