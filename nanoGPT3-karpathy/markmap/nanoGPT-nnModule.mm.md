---
markmap:
   colorFreezeLevel: 2
   maxWidth: 0
---

# NN.MODULE

## INIT

- **nn.Module**
  - -- imports --
    - import torch
    - import torch.nn as nn
    - from torch.nn import functional as F
      - ==[ @audit ]== : what is the purpose of `F`???
    - torch.manual_seed(1337)
      - // set the seed for generating random numbers
      - // we are manually setting to `1337` for reproducibility
- -- class --
  - ==[ BigramLanguageModel ğŸ§  ]==(nn.Module):
    - `def`
      - ğŸ”° **`__init__`** (self, vocab_size):
        - **vocab_size** is the size of the vocabulary the model will work with

        - ```python
            super().__init__()
            self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)
          ```

          - ==[ token_embedding_table ]==
            - -- args --
              - **num_embeddings**
              - **embedding_dim**
                - **vocab_size** per embedding
            - -- side effect --
              - creates **embedding layer**
                - a **lookup table** for each **vocab token** and **idx** ğŸ‘ï¸â€ğŸ—¨ï¸
              - ğŸ‘ï¸â€ğŸ—¨ï¸ **==[ idx ]==**
                - index to **token** in the vocabulary
                  - where each type of toy are :
                    - car
                    - doll
                    - dice ...etc
                - and it's **vector representation**
                  - a special sticker that descibes the toy
                    - color,
                    - size,
                    - how often we play with it ...etc
                - | EXAMPLE | **gpt ğŸ¤–**
                  - **[ embedding ]**
                    - -- vocab + **idx** ğŸ‘ï¸â€ğŸ—¨ï¸ --
                      - idx = id + uid.vector

                      - ```sh
                          vocab  id  uid.vector
                          -----  --  -----------
                          King    0  [1.0,  0.7]
                          Queen   1  [1.0, -0.5]
                          Man     2  [0.0,  0.5]
                          Woman   3  [0.0, -0.5]
                        ```

                    - -- graph --
                      - When our computer sees the word "King"
                        - in addition to the letters K, I, N, G.
                        - it ALSO sees the point [1, 0.5] ...

                      - ```sh
                          King  (1,  0.7)      x
                          Man   (0,  0.5)     x

                          Queen (1, -0.5)  x
                          Woman (0, -0.5)  x
                        ```

                        - Looking at these
                        points on a graph :
                          - -- distance --
                            - **royalty**
                              - ~ : King  -> Man
                              - ~ : Queen -> Woman
                            - **gender**
                              - ~ : King -> Queen
                              - ~ : Man  -> Woman
                            - word **algebra**
                              - King - Man + Woman = Queen
                          - -- horizontal --
                            - King and Queen match
                            - Man  and Woman match
                          - -- vertical --
                            - King  and Man   aligned **approximately**
                            - Queen and Woman aligned **exactly**
                          - -- ß· ğŸ§  ß· --
                            - âœ…  -- Queens are ONLY FEMALE
                            - âœ…  -- Kings aren't exclusively MALE
                          - -- other **semantic relationships** --
                            - **[ Remember ]**
                              - These semantic relationships are **not explicitly programmed** into the model
                              - They are **learned from the data the model is trained on**. The model notices
                              that certain words often appear in similar contexts, and it uses this information
                                to **place similar words close together in the embedding space**
                              - This ability to **learn semantic relationships from data** is one of the things that
                              **makes word embeddings so powerful**
                            - synonyms
                            - antonyms
                            - analogies
                            - part-whole relationships
                            - categories

      - â© **`forward`** (self, idx ğŸ‘ï¸â€ğŸ—¨ï¸, targets=None):
        - // forward() method defines how input is
        passed through the layers of the network

        - ```python
            logits = self.token_embedding_table(idx)  # (B,T,C)
            if targets == None:
              loss = None
            else:
              B, T, C = logits.shape
              logits = logits.view(B * T, C)
              targets = targets.view(B * T)
              loss = F.cross_entropy(logits, targets)
          ```

          - ğŸ§® **==[ logits ]==**
            - **logits** = self.token_embedding_table(idx)
              - // (B, T, C)
            - logits.**shape**
              - // manually unpacking B, T, C channels
              - | EXAMPLE |
                - // from xb.shape -- ğŸ“¥ inputs

                - ```python
                  tensor([
                          [53, 59,  6,  1, 58, 56, 47, 40], # Sequence 1
                          [49, 43, 43, 54,  1, 47, 58,  1], # Sequence 2
                          [13, 52, 45, 43, 50, 53,  8,  0], # Sequence 3
                          [ 1, 39,  1, 46, 53, 59, 57, 43]  # Sequence 4
                        ])
                  ```

              - ğŸªº ==[ B ]==
                - ğŸªº **batch_size**
                  - num_rows = [ 4 ]
                    - 4 **sentences** (per batch)
              - ğŸ¥š ==[ T ]==
                - ğŸ¥š **block_size**
                  - num_cols = [8]
                    - 8 **words** (sequence length per sentences)
              - ğŸ·ï¸ ==[ C ]==
                - ğŸ·ï¸ **vocab_size**
                  - num_ids = [65]
                    - 65 **unique ids** (vocabulary)
            - **logits**
              - // reshaping from **3D** to **2D** tensor
                - results in a single row of :
                  - logits (raw scores)
                    - **32 words** (4 * 8)
                  - per **token id**
                    - 65
                - This reshaping is done because
                **F.cross_entropy** expects
                  - ğŸ§® logits (ğŸ“¥ input) @audit ... input v logit
                    - **2D** tensor
                  - ğŸ¯ loss (targets)
                    - **1D** tensor
                - // Does NOT help GENERALIZE learning even though it removes batch (sentences) and unfolds to purely words (blocks) @mike
              - logits.**view( B * T, C)**
                - ( ğŸªº batch_size * ğŸ¥š block_size, ğŸ·ï¸ vocab_size)
                - ( **4 * 8** , 65 ) = ( **32***  65 )
          - `if` targets ğŸ‘€ == **None**:
            - // This case might happen during inference, when
              we don't have or need target values.
            - loss ğŸª¬ = **None**
          - `else:`
            - Calculate the loss if **targets** are provided
            - ğŸ‘€ **==[ targets ]==**
              - targets.**view(B * T)**
                - results in a 1D tensor : (B, T) => (B * T)
                  - where each element is the "true next token"
            - ğŸª¬ **==[ loss ]==**
              - loss = **F.cross_entropy**( ğŸ§® logits, ğŸ‘€ targets )
                - reshaped **logits** and **targets** are passed
                to the **cross entropy** loss function
                - This computes the **loss between** the **network's
                predictions** and the **actual targets**

        - `return` logits ğŸ§®, loss ğŸª¬
          - Finally, the **logits** and the **loss** are
            **returned** from the forward function
            - ğŸ§® **logits** can be used to **generate predictions**
            - ğŸª¬ **loss** is used during training to **update the model's weights**
      - ß· **`generate`** (self, idx, max_new_tokens):

        - ```python
            for _ in range(max_new_tokens):
              logits, loss = self(idx)
              logits = logits[:, -1, :]  # becomes (B, C)
              probs = F.softmax(logits, dim=1)  # (B, C)
              idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)
              idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)
          ```

        - `return` idx ğŸ‘ï¸â€ğŸ—¨ï¸
- -- main --
  - m = BigramLanguageModel(65)
    - ğŸ§  **==[ m ]==**
      - `m.forward(xb, yb)`
        - // pytorch allows us to call a model like a function
        - // - this is the same as calling m.forward(xb, yb)
          - xb = ğŸ“¥ inputs
          - yb = ğŸ¯ targets
    - **65** == ğŸ·ï¸ **vocab_size**
  - logits, loss = m( ğŸ“¥ xb, ğŸ¯ yb)
    - ==[ logits ]== ğŸ§®
      - logits.shape : torch.Size([32, 65]
        - **-- measure prediction --**
        - used to **measure** the model's prediction
        - | EXAMPLE | **gpt ğŸ¤–**
          - -- graph --
            - classify [inputs] [3 batches] => [classes] [5]
            - here is [batch size of 3] and a [vocabulary size of 5]
              - **row**
                - data point in batch [3]
              - **column**
                - [5] class to assign to

          - ```python
              tensor([[ 0.3134, -0.1676,  0.3773, -0.0824, -0.2973],
                      [-0.0856,  0.0987,  0.1772,  0.0565, -0.2135],
                      [ 0.0624,  0.0895,  0.0927, -0.0507, -0.1921]])
            ```

            - | RAW + **UNBOUNDED** |
              - values have no **min/max**
              - and can be positive or negative

          - ```python
              tensor([[0.2175, 0.1338, 0.2301, 0.1450, 0.2736],
                      [0.1823, 0.2194, 0.2372, 0.2109, 0.1502],
                      [0.2108, 0.2171, 0.2182, 0.1882, 0.1657]])
            ```

            - | **NORMALIZED** SOFTMAX |
              - `F.softmax(logits, dim=1)`
              - **convert to probabilities** via softmax
                - all values **between 0 and 1**
                - all **rows sum to 1.0**

    - ==[ loss ]== ğŸª¬
      - tensor(**4.8948**, grad_fn=<**'NllLossBackward0'**>)
        - **-- update weight --**
        - **NllLossBackward0** (backpropagation)
        - used to update model's **weight** during training

## PRE-TRAIN

- -- generate tensor --
  - ==[ idx ]== ğŸ‘ï¸â€ğŸ—¨ï¸
    - | EXAMPLE | **gpt ğŸ¤–**
      - **['the', 'cat', 'sat', 'on', 'mat']**
        - // language model that works with
        a [ vocabulary ] of [ 5 ] words
      - **idx** ğŸ‘ï¸â€ğŸ—¨ï¸ = torch.tensor([[0, 1]])
        - // **idx** representing "The cat"
        - // **2D tensor** with **shape (1, 2)**
          - [1] == batch_size (one row)
          - [2] == sequence length, "The" + "cat" == [ 2 tokens ]
      - **-- generate --**
        - **idx** ğŸ‘ï¸â€ğŸ—¨ï¸ = torch.tensor([[0, 1, 2, 3, 4]])
          - // After generation, idx might
          - // representing "The cat sat on mat"
          - // **2d tensor** updated to **shape (1, 5)**
            - (1, 2) "The cat"
            - (1, 5) "The cat sat on a mat"
    - **(B,T)** array of indices in the current context
      - **B** is the ğŸªº **batch_size**
        - **[8]** in this context
      - **T** is the **length** of **each sequence**
        - not same as ğŸ¥š **block_size**
          - T is the current sequence length of the context during
          generation, and it can change as new tokens are generated
          - ğŸ›‘ @audit : I don't understand this lol
    - `idx = torch.zeros((1, 1), dtype=torch.long)`
      - creates a 2D tensor full of zeros with shape (1, 1)
        - **idx** ğŸ‘ï¸â€ğŸ—¨ï¸ is used as the **initial sequence of tokens** to
        feed into the model for generating new tokens.
      - (token indices) tensor is of:
        - **shape (1, 1)**
          - because it is **intended to hold a sequence of tokens**
        - **type long**
          - because it is **intended to hold integer** values
- -- print tensor --
  - `print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))`

  - ```sh
      SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp
      wnYWmnxKWWev-tDqXErVKLgJ
    ```

    - // output is random garbage because
    we haven't trained the model yet
