{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KARPATHY NANO-GPT FOLLOW ALONG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Utilities Import ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# @note üß† : In Jupyter cell use `Path.cwd().parent` to get the parent directory \n",
    "# of the current working directory\n",
    "parent_directory = str(Path.cwd().parent)\n",
    "if parent_directory not in sys.path:\n",
    "    sys.path.append(parent_directory)\n",
    "\n",
    "from utils.dataframe.util_dataframe_table import UtilDataFrameTable as df_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- [PLAYGROUND] -- character prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Read Data From File ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters : 1115394\n"
     ]
    }
   ],
   "source": [
    "# read it to inspect it\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"length of dataset in characters : {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:100])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Gather Unique Character Sets ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[characters]--\n",
      "- 1st entry is the [space] character\n",
      "- 2nd entry is the [newline] character\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "--[vocab size]--\n",
      "\n",
      "  65\n"
     ]
    }
   ],
   "source": [
    "# Return a new list containing all items from the iterable in ascending order\n",
    "# - sorted() returns a new sorted list from an iterable\n",
    "# - if text = 'Hello, world' the resulting chars list will be :\n",
    "# [' ', ',', 'H', 'd', 'e', 'l', 'o', 'r', 'w']\n",
    "chars = sorted(\n",
    "    # Build an unordered collection of unique elements\n",
    "    # - create a set from the 'text', an unordered collection of unique elements\n",
    "    # ALL duplicate characters are removed from 'text' to achieve this\n",
    "    set(text)\n",
    ")\n",
    "# here are all the unique characters in the file\n",
    "print(\n",
    "    \"--[characters]--\"\n",
    "    \"\\n- 1st entry is the [space] character\"\n",
    "    \"\\n- 2nd entry is the [newline] character\"\n",
    "    f\"\\n{''.join(chars)}\\n\"\n",
    ")\n",
    "print(\"--[vocab size]--\\n\" f\"\\n  {len(chars)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Tokenize ] * --\n",
    "\n",
    "`Mapping` from `Characters to Integers` tradeoffs :\n",
    "\n",
    "- `CHARACTER` to integer mapping => short integer sequence, LARGER vocabulary  \n",
    "- `SUBWORD` tokenization => long integer sequence, SMALLER vocabulary\n",
    "  \n",
    "For this exercise, we will favor simpler `CHARACTER` tokenization, however large  \n",
    "production systems would likely use `SUBWORD` tokenization\n",
    "\n",
    "These mappings can be used to convert a string into a sequence of integers or  \n",
    "vice versa, which is `useful for working with machine learning models` that  \n",
    "`require numerical input`. \n",
    "\n",
    "> Note that the code snippet comments mention subword tokenization as an  \n",
    "alternative technique often used in large production systems.\n",
    "\n",
    "Subword tokenization can create :  \n",
    "- a smaller vocabulary size  \n",
    "- while preserving the information about common subwords and morphemes in the  \n",
    "text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:pad_value  is not convertible to int : setting to default 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[itos]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>!</td>\n",
       "      <td>3</td>\n",
       "      <td>$</td>\n",
       "      <td>4</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>5</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>,</td>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "      <td>8</td>\n",
       "      <td>.</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>:</td>\n",
       "      <td>11</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>?</td>\n",
       "      <td>13</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>B</td>\n",
       "      <td>15</td>\n",
       "      <td>C</td>\n",
       "      <td>16</td>\n",
       "      <td>D</td>\n",
       "      <td>17</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>G</td>\n",
       "      <td>20</td>\n",
       "      <td>H</td>\n",
       "      <td>21</td>\n",
       "      <td>I</td>\n",
       "      <td>22</td>\n",
       "      <td>J</td>\n",
       "      <td>23</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>L</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>N</td>\n",
       "      <td>27</td>\n",
       "      <td>O</td>\n",
       "      <td>28</td>\n",
       "      <td>P</td>\n",
       "      <td>29</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>R</td>\n",
       "      <td>31</td>\n",
       "      <td>S</td>\n",
       "      <td>32</td>\n",
       "      <td>T</td>\n",
       "      <td>33</td>\n",
       "      <td>U</td>\n",
       "      <td>34</td>\n",
       "      <td>V</td>\n",
       "      <td>35</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36</td>\n",
       "      <td>X</td>\n",
       "      <td>37</td>\n",
       "      <td>Y</td>\n",
       "      <td>38</td>\n",
       "      <td>Z</td>\n",
       "      <td>39</td>\n",
       "      <td>a</td>\n",
       "      <td>40</td>\n",
       "      <td>b</td>\n",
       "      <td>41</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>d</td>\n",
       "      <td>43</td>\n",
       "      <td>e</td>\n",
       "      <td>44</td>\n",
       "      <td>f</td>\n",
       "      <td>45</td>\n",
       "      <td>g</td>\n",
       "      <td>46</td>\n",
       "      <td>h</td>\n",
       "      <td>47</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48</td>\n",
       "      <td>j</td>\n",
       "      <td>49</td>\n",
       "      <td>k</td>\n",
       "      <td>50</td>\n",
       "      <td>l</td>\n",
       "      <td>51</td>\n",
       "      <td>m</td>\n",
       "      <td>52</td>\n",
       "      <td>n</td>\n",
       "      <td>53</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54</td>\n",
       "      <td>p</td>\n",
       "      <td>55</td>\n",
       "      <td>q</td>\n",
       "      <td>56</td>\n",
       "      <td>r</td>\n",
       "      <td>57</td>\n",
       "      <td>s</td>\n",
       "      <td>58</td>\n",
       "      <td>t</td>\n",
       "      <td>59</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>v</td>\n",
       "      <td>61</td>\n",
       "      <td>w</td>\n",
       "      <td>62</td>\n",
       "      <td>x</td>\n",
       "      <td>63</td>\n",
       "      <td>y</td>\n",
       "      <td>64</td>\n",
       "      <td>z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2  3   4  5   6  7   8  9   10 11\n",
       "0    0  \\n   1      2  !   3  $   4  &   5  '\n",
       "1    6   ,   7  -   8  .   9  3  10  :  11  ;\n",
       "2   12   ?  13  A  14  B  15  C  16  D  17  E\n",
       "3   18   F  19  G  20  H  21  I  22  J  23  K\n",
       "4   24   L  25  M  26  N  27  O  28  P  29  Q\n",
       "5   30   R  31  S  32  T  33  U  34  V  35  W\n",
       "6   36   X  37  Y  38  Z  39  a  40  b  41  c\n",
       "7   42   d  43  e  44  f  45  g  46  h  47  i\n",
       "8   48   j  49  k  50  l  51  m  52  n  53  o\n",
       "9   54   p  55  q  56  r  57  s  58  t  59  u\n",
       "10  60   v  61  w  62  x  63  y  64  z   0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Creates a dictionary of character to integer key/value pairs\n",
    "# - using dictionary comprehension, the enumerate() function is used to loop\n",
    "# over the 'chars' list and returns both [key]=character and [value]=ndex\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # string to integer\n",
    "# itos simply reverses the key/value pairs of the stoi dictionary\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # integer to string\n",
    "\n",
    "flatten_array = [entry for pair in itos.items() for entry in pair]\n",
    "table = df_table.array_to_table(np.array(flatten_array))\n",
    "df_table.show_table(pd.DataFrame(table), label=\"itos\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [Encode and Decode] * --\n",
    "- Character : Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 43, 43, 9]\n",
      "Bee3\n",
      "--[ERROR]--\n",
      "'7'\n"
     ]
    }
   ],
   "source": [
    "# lambda function uses list comprehension to process each item in input and\n",
    "# return a list of output items\n",
    "\n",
    "# Encoder : takes a string (list of characters) and returns a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# Decoder :takes a list of integers and outputs a string (list of characters)\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# input_phrase = 'six eggs and avocado skins forever lol'\n",
    "input_phrase = \"Bee3\"\n",
    "print(encode(input_phrase))\n",
    "print(decode(encode(input_phrase)))\n",
    "\n",
    "# our shakespeare text doesn't have any numerical characters other than 3\n",
    "# so this should error out\n",
    "input_phrase = \"Bee7\"\n",
    "try:\n",
    "    print(encode(input_phrase))\n",
    "except KeyError as e:\n",
    "    print(f\"--[ERROR]--\\n{e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Generate Data Tensor ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[data.shape]-- torch.Size([1115394])\n",
      "--[data.dtype]-- torch.int64\n",
      "\n",
      "# the 15 characters we looked at earlier to GPT look like this :\n",
      "\n",
      "First Citizen:\n",
      "\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])\n",
      "\n",
      "--[18]-- F\n",
      "--[47]-- i\n",
      "--[56]-- r\n",
      "--[57]-- s\n",
      "--[58]-- t\n",
      "--[1]--  \n",
      "--[15]-- C\n",
      "--[47]-- i\n",
      "--[58]-- t\n",
      "--[47]-- i\n",
      "--[64]-- z\n",
      "--[43]-- e\n",
      "--[52]-- n\n",
      "--[10]-- :\n",
      "--[0]-- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's encode the entire shakespeare text dataset and store it in a torch.Tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "print(\n",
    "    f\"--[data.shape]-- {data.shape}\"\n",
    "    f\"\\n--[data.dtype]-- {data.dtype}\"\n",
    "    \"\\n\\n# the 15 characters we looked at earlier to GPT look like this :\"\n",
    "    f\"\\n\\n{text[:15]}\"\n",
    "    f\"\\n{data[:15]}\\n\"\n",
    ")\n",
    "\n",
    "for entry in data[:15]:\n",
    "    print(f\"--[{entry}]-- {itos[int(entry)]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Split Training and Validation Sets ] *--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split up the data into train and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]  # first 90% of the data is training data\n",
    "val_data = data[n:]  # the rest is for validation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **-- [Block Size] --**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting the training data, you use `train_data[:block_size+1]` because  \n",
    "you want to `predict` the `next` character after the `CURRENT BLOCK` from the  \n",
    "`NEIGHBORING BLOCK`\n",
    "\n",
    "By including one extra character, you provide the model with `both` the : \n",
    "\n",
    "- `input` sequence (the first 8 characters) and  \n",
    "- the `target` character (the 9th character) to predict. \n",
    "\n",
    "This allows the model to `learn` the relationship between the `input` sequence  \n",
    "and the `subsequent` character, which is essential for character-level language  \n",
    "modeling tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of `block_size` is a trade-off because:\n",
    "\n",
    "1. Processing `8 characters` at a time allows the model to learn relationships  \n",
    "between characters within `shorter sequences`, which can help with  \n",
    "`generalization` and computational `efficiency`.\n",
    "2. Using a larger `block_size` would let the model learn longer-range  \n",
    "dependencies between characters, but it would also increase the memory  \n",
    "requirements and computational complexity of the model.\n",
    "\n",
    "In this example, the training data is sliced to include `block_size + 1`  \n",
    "characters, which is 9 characters in total. The `reason for adding 1` to the  \n",
    "block size is that the model needs to `predict the next character` given the  \n",
    "current sequence.  \n",
    "\n",
    "So, the first `8 characters are the input sequence`, and the `9th character` is  \n",
    "the `target to be predicted`.\n",
    "\n",
    "Here's a brief explanation of the code:\n",
    "\n",
    "- **`train_data`**: This is the entire training dataset containing character  \n",
    "sequences.\n",
    "- **`block_size`**: The number of characters to be considered at a time during  \n",
    "training.\n",
    "- **`train_data[:block_size+1]`**: This slice of the training data includes the  \n",
    "first 9 characters. The first `8 characters are the input`, and the  \n",
    "`9th character` is the `target to be predicted` by the model.\n",
    "\n",
    "In summary, the `block_size` is set to 8 to balance the trade-off between  \n",
    "processing characters in shorter sequences and learning relationships between  \n",
    "characters in the text. The additional character added to the block size is  \n",
    "needed for the model to predict the next character in the sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher res notes on **`BLOCK`** and **`BATCH`** size trade offs \n",
    "***\n",
    "Setting the `batch_size` and `block_size` in a character-based language model  \n",
    "can affect the training and performance of the model. Here's an explanation of  \n",
    "the trade-offs and other considerations when choosing these values:\n",
    "\n",
    "1. **Batch size (`batch_size`):** The number of independent sequences processed  \n",
    "in parallel.\n",
    "\n",
    "   - *Pros:*\n",
    "     - Larger batch sizes can lead to faster training as the model processes  \n",
    "     more samples in parallel. This can potentially lead to better utilization  \n",
    "     of GPU resources.\n",
    "     - Larger batch sizes can provide a better estimate of the gradient during  \n",
    "     each training step, which can lead to more stable convergence.\n",
    "\n",
    "   - *Cons:*\n",
    "     - Larger batch sizes require more memory. If the batch size is too large,  \n",
    "     it may not fit in the GPU memory, leading to out-of-memory errors.\n",
    "     - Sometimes, larger batch sizes can lead to worse generalization  \n",
    "     performance. Smaller batch sizes introduce more `noise in the gradient`  \n",
    "     updates, which can effectively act as a `form of regularization`.\n",
    "\n",
    "2. **Block size (`block_size`):** The maximum context length for prediction.\n",
    "\n",
    "   - *Pros:*\n",
    "     - Larger block sizes allow the model to learn longer-range dependencies in  \n",
    "     the text, which can potentially improve modeling performance.\n",
    "     - Larger block sizes can lead to better utilization of GPU resources, as  \n",
    "     more character sequences are processed in parallel.\n",
    "\n",
    "   - *Cons:*\n",
    "     - Larger block sizes require more memory. If the block size is too large,  \n",
    "     it may not fit in the GPU memory, leading to out-of-memory errors.\n",
    "     - Longer sequences may be more difficult for the model to learn, as the  \n",
    "     gradients can have a harder time propagating through the entire sequence in  \n",
    "     the backpropagation process (vanishing/exploding gradient problem).\n",
    "\n",
    "It's essential to strike a balance between these trade-offs when choosing  \n",
    "`batch_size` and `block_size`. You also need to consider other factors, such as:\n",
    "\n",
    "- **Hardware limitations:** The GPU memory available on your system will limit  \n",
    "the maximum batch size and block size you can use. You need to choose values  \n",
    "that can efficiently utilize the available resources without causing  \n",
    "out-of-memory issues.\n",
    "\n",
    "- **Dataset size and complexity:** The size and complexity of your dataset can  \n",
    "affect the optimal batch size and block size. For larger and more complex  \n",
    "datasets, you may need to use larger block sizes to capture longer-range  \n",
    "dependencies.\n",
    "\n",
    "- **Model architecture:** The choice of model architecture can affect the  \n",
    "optimal batch size and block size. For example, some architectures like  \n",
    "`transformers can handle longer sequences more effectively` than recurrent  \n",
    "neural networks (RNNs), so you might consider using larger block sizes with  \n",
    "transformers.\n",
    "\n",
    "In summary, choosing the right `batch` size and `block` size involves balancing  \n",
    "various trade-offs :  \n",
    "\n",
    "- considering hardware limitations,  \n",
    "- dataset characteristics,  \n",
    "- and model architecture.  \n",
    "\n",
    "It's often helpful to experiment with different values to find the best   combination that works for your specific problem and setup."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Train Data Load ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `block_size` inlined to 8 because it's a good trade off to :\n",
    "# - model to process 8 characters at a time\n",
    "# - learn the relationships between characters in shorter sequences\n",
    "block_size = 8  # characters to consider at a time, can't load entire file\n",
    "# @note üß† : Ah ok, so we ARE actually INCLUDING a CHARACTER from the NEIGHBORING\n",
    "# BLOCK in the TRAINING DATA\n",
    "train_data[: block_size + 1]  # +1 because we need to predict the next character\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- | Predicting Batches |--\n",
    "\n",
    "Predicting (even across `BATCHES`) the next character in a sequence :\n",
    "\n",
    "- Using a `sliding window` approach to demonstrates how to create input-output  \n",
    "pairs for a character-level language model\n",
    "- Which considers both the context of characters in the input sequence and the  \n",
    "next character to be predicted \n",
    "- By `iterating` through the `input sequence` and `creating` `input-output pairs` for  \n",
    "`each position`, the model learns to `predict` the `next character` based on the  \n",
    "context of `previously seen characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- torch.Size([8]) -- \n",
      "-- [x_0] -- batch => (train_data[:block_size])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>i</td>\n",
       "      <td>r</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4  5   6   7\n",
       "0  18  47  56  57  58  1  15  47\n",
       "0   F   i   r   s   t      C   i"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- [x_1] -- batch => (train_data[block_size :block_size*2])\n",
      "tensor([58, 47, 64, 43, 52, 10,  0, 14])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t</td>\n",
       "      <td>i</td>\n",
       "      <td>z</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>:</td>\n",
       "      <td>\\n</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7\n",
       "0  58  47  64  43  52  10   0  14\n",
       "0   t   i   z   e   n   :  \\n   B"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @t   context | x_0 | target | y[t] |\n",
      "----  -------         ------\n",
      "[ 0]     18     [F]     47     [i]    => tensor([18]) the target is 47\n",
      "[ 1]     47     [i]     56     [r]    => tensor([18, 47]) the target is 56\n",
      "[ 2]     56     [r]     57     [s]    => tensor([18, 47, 56]) the target is 57\n",
      "[ 3]     57     [s]     58     [t]    => tensor([18, 47, 56, 57]) the target is 58\n",
      "[ 4]     58     [t]      1     [ ]    => tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "[ 5]      1     [ ]     15     [C]    => tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "[ 6]     15     [C]     47     [i]    => tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "[ 7]     47     [i]     58     [t]    => tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# The objective of the model is to predict the next character in a sequence\n",
    "# given a context of previous characters\n",
    "# TIME dimension : series of characters that are dependent on each other\n",
    "\n",
    "# x_0 is the first `block_size` character batch from the `train_data`\n",
    "# x_1 is the next 'block_size' character batch from `train_data`\n",
    "x_0 = train_data[:block_size]\n",
    "x_1 = train_data[block_size :block_size*2]\n",
    "print(f\"-- {x_0.shape} -- \\n\" f\"-- [x_0] -- batch => (train_data[:block_size])\\n{x_0}\")\n",
    "\n",
    "# @note üß† : passing -1 to reshape() will infer the correct dimension given :\n",
    "# - the number of total elements in the tensor\n",
    "# - the number of other dimensions that are specified\n",
    "# Also we are inlining itos[] dictionary lookup function to the lambda function\n",
    "df_table.show_table(x_0.reshape(-1, block_size), func=lambda x: itos[x])\n",
    "\n",
    "print(f\"-- [x_1] -- batch => (train_data[block_size :block_size*2])\\n{x_1}\\n\")\n",
    "\n",
    "df_table.show_table(x_1.reshape(-1, block_size), func=lambda x: itos[x])\n",
    "\n",
    "# CREATES the output sequence `y` by shifting the `train_data` by one position\n",
    "# to the right, effectively creating a sequence with the next character for\n",
    "# each position in `x`\n",
    "# - +1 to both start/end because we want to predict the next character\n",
    "y = train_data[1 : block_size + 1]\n",
    "# loop iterates over each position `t` in the input sequence `x\n",
    "print(f\" @t   context | x_0 | target | y[t] |\\n\" \"----  -------         ------\")\n",
    "for t in range(block_size):\n",
    "    # the `context` is created by taking the characters from BEGINNING of `x`\n",
    "    # up to and INCLUDING the character at position `t`.\n",
    "    context = x_0[: t + 1]\n",
    "    # the corresponding target character for the current `context` is the\n",
    "    # character at position `t` in the output sequence `y`\n",
    "    target = y[t]\n",
    "    print(\n",
    "        f\"[{t:2}] {context[-1]:6}     [{itos[int(context[-1])]}]  {target:5}     [{itos[int(target)]}]\"\n",
    "        f\"    => {context} the target is {target}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet shows how to create a batch of input-output pairs for training or validation from the dataset using a random sampling approach. It defines a function `get_batch(split)` that generates a small batch of data based on the specified `split` ('train' or 'val'). Let's break down the code step by step.\n",
    "\n",
    "1. `torch.manual_seed(1337)`: This line sets the random seed for PyTorch to ensure reproducibility of the results.\n",
    "\n",
    "2. `batch_size` and `block_size` are set to 4 and 8, respectively. `batch_size` indicates the number of independent sequences to process in parallel, and `block_size` defines the maximum context length for prediction.\n",
    "\n",
    "3. `get_batch(split)` function:\n",
    "   - `data = train_data if split == 'train' else val_data`: This line sets the data source based on the specified `split`. If `split` is 'train', the `data` variable will be set to `train_data`. Otherwise, the `data` variable will be set to `val_data`.\n",
    "   \n",
    "   - `ix = torch.randint(len(data)-block_size-1, (batch_size,))`: This line generates a random offset within the dataset for each sequence in the batch. The offset is chosen so that the selected sequences do not exceed the dataset's length.\n",
    "   \n",
    "   - `x = torch.stack([data[i:i+block_size] for i in ix])`: This line creates a 2D tensor `x` containing the input sequences. For each random offset `i` in `ix`, it extracts a sequence of length `block_size` from the `data`, stacks these sequences as rows, and forms a 2D tensor of shape `(batch_size, block_size)`, i.e., 4 rows of 8 characters.\n",
    "   \n",
    "   - `y = torch.stack([data[i+1:i+block_size+1] for i in ix])`: This line creates a 2D tensor `y` containing the target sequences. It is similar to the previous line, but the extracted sequences are shifted by one character to the right, effectively creating a sequence of target characters for each input sequence in `x`.\n",
    "   \n",
    "The `get_batch` function returns a pair of tensors, `x` and `y`, which represent the input-output pairs for the character-level language model. This function allows you to generate batches of data for training or validation in a memory-efficient way using random sampling.\n",
    "\n",
    "### -- * [ Get Batch Method ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# BATCH dimension : segments of chunk that are totally independent of each other\n",
    "torch.manual_seed(1337)  # inlined to 'leet' to ensure reproducibility of the results\n",
    "batch_size = 4  # how many independent sequences will we process in parallel\n",
    "block_size = 8  # what is the maximum context length for prediction\n",
    "\n",
    "\n",
    "# generate a small batch of data and returns 2 tensors : \n",
    "# - (inputs) x \n",
    "# - (targets) y\n",
    "def get_batch(split) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate batches of data for training or validation in a memory-efficient\n",
    "    way using random sampling :\n",
    "    - `split` ('train' or 'val') generate data batch for TRAINING or VALIDATION (default)\n",
    "    - returns a pair of tensors, `x` and `y`, which represent the input-output\n",
    "    pairs for the character-level language model\n",
    "    \"\"\"\n",
    "    # @audit üçù : `train_data` and `val_data` are GLOBAL variables\n",
    "    data = train_data if split == \"train\" else val_data  # default == validation\n",
    "    # Randomize offset to prevent Model from MEMORIZING character POSITION\n",
    "    # - so we can GENERALIZE to ONLY neighboring characters\n",
    "    # generate random offset for each sequence in the batch ...\n",
    "    # - DO NOT EXCEED unit BATCH SIZE\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    # creates a 2D tensor `x` containing the input sequences, where :\n",
    "    # - for each random offset `i` in `ix`, it extracts a sequence of length\n",
    "    # `block_size` from the `data`\n",
    "    # - stacks these sequences as rows\n",
    "    # - and forms a 2D tensor of shape `(batch_size, block_size)`\n",
    "    # i.e.4 rows of 8 characters\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # y = next character i.e. x shifted by 1\n",
    "    # - effectively creating a sequence of target characters for each input\n",
    "    # sequence in `x`\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Instantiate xb (input) yb (targets) ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[inputs]--\n",
      " xb.shape : torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o</td>\n",
       "      <td>u</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>i</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k</td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>p</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>t</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>h</td>\n",
       "      <td>o</td>\n",
       "      <td>u</td>\n",
       "      <td>s</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7\n",
       "0  53  59   6   1  58  56  47  40\n",
       "0   o   u   ,       t   r   i   b\n",
       "1  49  43  43  54   1  47  58   1\n",
       "1   k   e   e   p       i   t    \n",
       "2  13  52  45  43  50  53   8   0\n",
       "2   A   n   g   e   l   o   .  \\n\n",
       "3   1  39   1  46  53  59  57  43\n",
       "3       a       h   o   u   s   e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[targets]--\n",
      " yb.shape : torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>i</td>\n",
       "      <td>b</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>p</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>t</td>\n",
       "      <td></td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>h</td>\n",
       "      <td>o</td>\n",
       "      <td>u</td>\n",
       "      <td>s</td>\n",
       "      <td>e</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7\n",
       "0  59   6   1  58  56  47  40  59\n",
       "0   u   ,       t   r   i   b   u\n",
       "1  43  43  54   1  47  58   1  58\n",
       "1   e   e   p       i   t       t\n",
       "2  52  45  43  50  53   8   0  26\n",
       "2   n   g   e   l   o   .  \\n   N\n",
       "3  39   1  46  53  59  57  43   0\n",
       "3   a       h   o   u   s   e  \\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\")\n",
    "print(f\"--[inputs]--\\n xb.shape : {xb.shape}\\n{xb}\")\n",
    "df_table.show_table(xb, func=lambda x: itos[x])\n",
    "\n",
    "\n",
    "# @audit üõë : Explain how the last element of each yb row is generated\n",
    "print(f\"--[targets]--\\n yb.shape : {yb.shape}\\n{yb}\")\n",
    "df_table.show_table(yb, func=lambda x: itos[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input (context) is tensor([53]) the target is 59\n",
      "when input (context) is tensor([53, 59]) the target is 6\n",
      "when input (context) is tensor([53, 59,  6]) the target is 1\n",
      "when input (context) is tensor([53, 59,  6,  1]) the target is 58\n",
      "when input (context) is tensor([53, 59,  6,  1, 58]) the target is 56\n",
      "when input (context) is tensor([53, 59,  6,  1, 58, 56]) the target is 47\n",
      "when input (context) is tensor([53, 59,  6,  1, 58, 56, 47]) the target is 40\n",
      "when input (context) is tensor([53, 59,  6,  1, 58, 56, 47, 40]) the target is 59\n",
      "when input (context) is tensor([49]) the target is 43\n",
      "when input (context) is tensor([49, 43]) the target is 43\n",
      "when input (context) is tensor([49, 43, 43]) the target is 54\n",
      "when input (context) is tensor([49, 43, 43, 54]) the target is 1\n",
      "when input (context) is tensor([49, 43, 43, 54,  1]) the target is 47\n",
      "when input (context) is tensor([49, 43, 43, 54,  1, 47]) the target is 58\n",
      "when input (context) is tensor([49, 43, 43, 54,  1, 47, 58]) the target is 1\n",
      "when input (context) is tensor([49, 43, 43, 54,  1, 47, 58,  1]) the target is 58\n",
      "when input (context) is tensor([13]) the target is 52\n",
      "when input (context) is tensor([13, 52]) the target is 45\n",
      "when input (context) is tensor([13, 52, 45]) the target is 43\n",
      "when input (context) is tensor([13, 52, 45, 43]) the target is 50\n",
      "when input (context) is tensor([13, 52, 45, 43, 50]) the target is 53\n",
      "when input (context) is tensor([13, 52, 45, 43, 50, 53]) the target is 8\n",
      "when input (context) is tensor([13, 52, 45, 43, 50, 53,  8]) the target is 0\n",
      "when input (context) is tensor([13, 52, 45, 43, 50, 53,  8,  0]) the target is 26\n",
      "when input (context) is tensor([1]) the target is 39\n",
      "when input (context) is tensor([ 1, 39]) the target is 1\n",
      "when input (context) is tensor([ 1, 39,  1]) the target is 46\n",
      "when input (context) is tensor([ 1, 39,  1, 46]) the target is 53\n",
      "when input (context) is tensor([ 1, 39,  1, 46, 53]) the target is 59\n",
      "when input (context) is tensor([ 1, 39,  1, 46, 53, 59]) the target is 57\n",
      "when input (context) is tensor([ 1, 39,  1, 46, 53, 59, 57]) the target is 43\n",
      "when input (context) is tensor([ 1, 39,  1, 46, 53, 59, 57, 43]) the target is 0\n"
     ]
    }
   ],
   "source": [
    "# print out the 32 independent examples (4 batches of 8 characters each)\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input (context) is {context} the target is {target}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ BigramLanguageModel Class ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pdb\n",
    "\n",
    "# set the seed for generating random numbers\n",
    "# - we are manually setting to `1337` for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# @audit-ok : result is not captured in variable so it is NOT USED in our code?\n",
    "# WE will comment it out @mike\n",
    "# nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    # contructor takes self and vocab_size as arguments\n",
    "    # - vocab_size is the number of unique tokens in the vocabulary\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # This line creates an embedding layer for the tokens in the vocabulary \n",
    "        # The embedding layer is essentially a lookup table where each token \n",
    "        # in the vocabulary is associated with a unique vector.\n",
    "        # @audit-ok : Explain this ^ like I am 5\n",
    "        # ANSWER\n",
    "        # vocab  id  uid.vector\n",
    "        # -----  --  -----------\n",
    "        # King    0  [1.0,  0.7]\n",
    "        # Queen   1  [1.0, -0.5]\n",
    "        # Man     2  [0.0,  0.5]\n",
    "        # Woman   3  [0.0, -0.5] \n",
    "        #\n",
    "        # ------ graph of the embedding table ----\n",
    "        #\n",
    "        # King  (1,  0.7)      x \n",
    "        # Man   (0,  0.5)     x\n",
    "        #\n",
    "        # Queen (1, -0.5)  x\n",
    "        # Woman (0, -0.5)  x\n",
    "        #\n",
    "        # ------ take a ways ----\n",
    "        #\n",
    "        # Looking at these points on a graph :\n",
    "        # - -- horizontal --\n",
    "        #   - \"King\" and \"Queen\" match\n",
    "        #   - \"Man\" and \"Woman\" match\n",
    "        # - -- vertical --\n",
    "        #   - \"King\" and \"Man\" aligned **approximately**\n",
    "        #   - \"Queen\" and \"Woman\" aligned **exactly**\n",
    "        # - -- correlations --\n",
    "        #   - Queens are ONLY FEMALE\n",
    "        #   - Kings aren't exclusively MALE\n",
    "        # TODO : when would num_embeddings != embedding_dim?\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "    # forward() method defines how input is passed through the layers of the \n",
    "    # network\n",
    "    def forward(self, idx, targets=None, debug=False):\n",
    "\n",
    "        # | POPULATE LOGITS | \n",
    "        # @audit we MUST do this even if early exit ... why??\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # @audit-ok : what are B and T and C?\n",
    "        # ANSWER \n",
    "        # B = batch (number of sentences [4]) \n",
    "        # T = tokens (number of words [8] per sentence)\n",
    "        # C = categories (unique vocabulary tokens [65])\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C) ‚ôø\n",
    "        if debug :\n",
    "            print(\"[BGL] forward() \\n idx.shape @ token_embedding_table : \", idx.shape)\n",
    "            print(\"[BGL] forward() \\n logits.shape @ token_embedding_table\", logits.shape)\n",
    "            print(\"[BGL] forward() \\n idx @ token_embedding_table : \", idx)\n",
    "            print(\"[BGL] forward() \\n logits @ token_embedding_table\", logits)\n",
    "\n",
    "        # | EARLY EXIT |\n",
    "        # branching to support targets=None for generate function to call self\n",
    "        # - This case might happen during inference, when we don't have or need \n",
    "        # target values.\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # | UNPACK AND CACULATE LOSS |\n",
    "            # Manually unpack channels so we can specify order that pytorch uses\n",
    "            B, T, C = logits.shape\n",
    "            # refactor to 2D array by unrolling the batch and time dimensions\n",
    "            logits = logits.view(B * T, C)\n",
    "            # targets.view(-1) also works, -1 means 'infer'\n",
    "            # TODO : understand how this -1 infer works\n",
    "            # @audit-ok : what is the difference between .view() and .reshape()?\n",
    "            # ANSWER\n",
    "            # Both are used to reshape the tensor, the difference is that\n",
    "            # - .view requires that tensor is contiguous in memory\n",
    "            # - .reshape will handle a non-contigous tensor, if necessary\n",
    "            # will copy the data to make it contigous, however this uses\n",
    "            # extra memory and may not be what you want\n",
    "            targets = targets.view(B * T)   # targets reshaped to (B * T, )\n",
    "                                            # a 1D tensor of integers\n",
    "            # ^^^ view/reshaping is to match cross entropy loss input \n",
    "            # requirements\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        if debug:\n",
    "            if targets == None:\n",
    "                print(\"[BLG] forward() return targets NONE\")\n",
    "            else:\n",
    "                print(\"[BGL] forward() \\n idx.shape : \", idx.shape)\n",
    "                print(\"[BGL] forward() return \\n targets.shape : \", targets.shape)\n",
    "                print(\"[BGL] forward() return \\n  : logits.shape \", logits.shape)\n",
    "                print(\"[BGL] forward() return \\n  :  loss.shape \", loss.shape)\n",
    "                print(\"[BGL] forward() \\n idx : \", idx)\n",
    "                print(\"[BGL] forward() return \\n targets : \", targets)\n",
    "                print(\"[BGL] forward() return \\n  : logits \", logits)\n",
    "                print(\"[BGL] forward() return \\n  :  loss \", loss)\n",
    "        return logits, loss\n",
    "\n",
    "    # generates a sequence of tokens\n",
    "    # - idx : input data, (B,T) array of indices token IDs to start generating\n",
    "    # the next tokens from\n",
    "    # - max_new_tokens : how many tokens to generate\n",
    "    # - returns idx : the generated sequence of tokens < `max_new_tokens`\n",
    "    def generate(self, idx, max_new_tokens, debug=False):\n",
    "        if debug:\n",
    "            logits, _ = self(idx)\n",
    "            print(\"[BGL] generate() \\n max_new_tokens : \", max_new_tokens)\n",
    "            print(\"[BGL] generate() \\n idx.shape : \", idx.shape)\n",
    "            print(\"[BGL] generate() \\n logits.shape : \", logits.shape)\n",
    "            print(\"[BGL] generate() \\n idx : \", idx)\n",
    "            print(\"[BGL] generate() \\n logits : \", logits)\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            # loss is ignored, only focusing on logits\n",
    "            # TODO : why is loss ignored? can we format to not include it?\n",
    "            # Also updating forward() to support default loss=None\n",
    "            # self(idx) is nn.Module.__call()__ which effectively calls\n",
    "            # - self.forward() with the same arguments : `idx`\n",
    "            # - + additional hooks before and after `forward()` is called\n",
    "            # - so not exactly the same as `self.forward()` but close enough\n",
    "            logits, _ = self(idx) # we only want the logits, loss will be NONE\n",
    "            # focus only on the last[-1] [T]ime step ... so essentially we are :\n",
    "            # (B, T, C) -> (B * T, C) where B * T = B * T[-1] ... B != T[-1]\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        # pdb.set_trace()\n",
    "        if debug:\n",
    "            print(\"[BGL] generate() \\n logits.shape : \", logits.shape)\n",
    "            print(\"[BGL] generate() \\n probs.shape : \", probs.shape)\n",
    "            print(\"[BGL] generate() \\n idx.shape : \", idx.shape)\n",
    "            print(\"[BGL] generate() \\n idx_next.shape : \", idx_next.shape)\n",
    "            print(\"[BGL] generate() \\n logits : \", logits)\n",
    "            print(\"[BGL] generate() \\n probs : \", probs)\n",
    "            print(\"[BGL] generate() \\n idx : \", idx)\n",
    "            print(\"[BGL] generate() \\n idx_next : \", idx_next)\n",
    "        return idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Instantiate BigramLanguage Model ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BGL] forward() \n",
      " idx.shape @ token_embedding_table :  torch.Size([4, 8])\n",
      "[BGL] forward() \n",
      " logits.shape @ token_embedding_table torch.Size([4, 8, 65])\n",
      "[BGL] forward() \n",
      " idx @ token_embedding_table :  tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "[BGL] forward() \n",
      " logits @ token_embedding_table tensor([[[-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
      "         [-0.4002,  0.3302,  1.5454,  ...,  1.3688,  0.4620,  0.2040],\n",
      "         [ 0.4160,  0.3362, -0.4512,  ..., -1.6525, -0.8816, -1.4546],\n",
      "         ...,\n",
      "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
      "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
      "         [ 1.4311,  0.4160, -2.2246,  ...,  0.7330,  0.3551,  0.1472]],\n",
      "\n",
      "        [[-0.2950, -0.6511,  1.4937,  ..., -0.5250,  1.2672,  2.6002],\n",
      "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "         ...,\n",
      "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
      "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275]],\n",
      "\n",
      "        [[ 0.2073,  0.8193,  0.3750,  ..., -0.9352, -0.6125, -0.3161],\n",
      "         [-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909],\n",
      "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948],\n",
      "         ...,\n",
      "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
      "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
      "         [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345]],\n",
      "\n",
      "        [[ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
      "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         ...,\n",
      "         [-0.4002,  0.3302,  1.5454,  ...,  1.3688,  0.4620,  0.2040],\n",
      "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "[BGL] forward() \n",
      " idx.shape :  torch.Size([4, 8])\n",
      "[BGL] forward() return \n",
      " targets.shape :  torch.Size([32])\n",
      "[BGL] forward() return \n",
      "  : logits.shape  torch.Size([32, 65])\n",
      "[BGL] forward() return \n",
      "  :  loss.shape  torch.Size([])\n",
      "[BGL] forward() \n",
      " idx :  tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "[BGL] forward() return \n",
      " targets :  tensor([59,  6,  1, 58, 56, 47, 40, 59, 43, 43, 54,  1, 47, 58,  1, 58, 52, 45,\n",
      "        43, 50, 53,  8,  0, 26, 39,  1, 46, 53, 59, 57, 43,  0])\n",
      "[BGL] forward() return \n",
      "  : logits  tensor([[-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
      "        [-0.4002,  0.3302,  1.5454,  ...,  1.3688,  0.4620,  0.2040],\n",
      "        [ 0.4160,  0.3362, -0.4512,  ..., -1.6525, -0.8816, -1.4546],\n",
      "        ...,\n",
      "        [-0.4002,  0.3302,  1.5454,  ...,  1.3688,  0.4620,  0.2040],\n",
      "        [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "[BGL] forward() return \n",
      "  :  loss  tensor(4.8948, grad_fn=<NllLossBackward0>)\n",
      "logits.shape : torch.Size([32, 65]) loss : 4.894842624664307\n",
      "tensor(4.8948, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# in video this value is vocab_size = 65, but here I'm hard coding by hand\n",
    "# TODO : review what I missed in video with vocab_size\n",
    "m = BigramLanguageModel(65)\n",
    "# calling the 'forward' method of the model\n",
    "# - pytorch allows us to call the model like a function\n",
    "# - this is the same as calling m.forward(xb, yb)\n",
    "logits, loss = m(xb, yb, debug=True)\n",
    "# - logits = measure model's prediction\n",
    "# - loss - used to update model's weights (backpropagation)\n",
    "\n",
    "# TODO : why is printing {loss} format only returns value and not tensor?\n",
    "print(f\"logits.shape : {logits.shape} loss : {loss}\")\n",
    "# given -ln(1/65) = 4.174387269895637\n",
    "print(loss)  # to check which grad_fn tensor is using\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- | PreTraining Generate | --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE training\n",
    "# creates a 2D tensor full of zeros with shape (1, 1)\n",
    "# - tensor is of **type long** because it is **intended to hold integer** values\n",
    "# - tensor is of **shape (1, 1)** because it is **intended to hold a sequence of \n",
    "# tokens**\n",
    "# - idx is used as the **initial sequence of tokens** to feed into the model for \n",
    "# generating new tokens.\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# output is random garbage because we haven't trained the model yet\n",
    "print(decode(m.generate(idx, max_new_tokens=100, debug=True)[0].tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Instantiate Optimizer] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch optimizer\n",
    "# TODO : what is AdamW? And why is it better than SGD?\n",
    "# Answer : the main difference between SGD and AdamW is :\n",
    "# - AdamW incorporates weight decay into the optimizer step size, weight decay is\n",
    "#   is a regularization technique that helps prevent overfitting by ensuring that\n",
    "#   weights remain close to their initial values\n",
    "# - AdamW also incorporates momentum into the optimizer step size -- TODO: what is momentum?\n",
    "# - SGD is STOCHASTIC, meaning that it takes small steps towards the minimum of\n",
    "#   the loss function, whereas AdamW is ADAPTIVE which takes larger steps AND\n",
    "#   adjusts step size according to the current state of the system\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- * [ Train Loop Cycle ] * --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6720, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # why did we change batch size from 4 to 32\n",
    "\n",
    "# @audit : This loop takes a ton of time??\n",
    "# @ 1,000 steps      : loss ~ 4.3 ... but repeatedly running can ~ 2.6\n",
    "                       # @audit ... is each step leaving RESIDUE to build up ???\n",
    "# - with batch = 4   : 1 seconds   tensor(2.6720, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "# @ 10,000 steps     : loss ~ 2.6\n",
    "# - with batch = 4   : 2.5 seconds   tensor(2.6720, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "# @ 100,000 steps    : loss ~ 2.4\n",
    "# - with batch = 4   : 22 seconds   tensor(2.4284, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "# @ 1,000,000 steps  : loss ~2.4\n",
    "# - with batch = 4   : 4 minutes seconds   tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "# @ 10,000,000 steps : loss ~2.5\n",
    "# - with batch = 4   : 37 minutes seconds   tensor(2.5116, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "# ANSWER\n",
    "# - yes for 100,000 steps : \n",
    "# - time : 1minute 17 seconds\n",
    "# - loss : ~ 2.406831979751587\n",
    "\n",
    "\n",
    "for steps in range(10000): # loss ~ 2.6\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb, debug=False)\n",
    "    # zeroing out gradient from previous step\n",
    "    # set_to_none\n",
    "    # - True  : biased to MEMORY efficiency, LARGER models are MEMORY gated\n",
    "    # - False : biased to CPU efficiency, uses more MEMORY by skipping clean up\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # getting gradient of loss wrt to model parameters\n",
    "    loss.backward()\n",
    "    # using gradient to update model parameters\n",
    "    # - embedded_table updated\n",
    "    # - @audit ... are weights and biases ALSO updated for BIGRAM model?\n",
    "    optimizer.step()\n",
    "\n",
    "if loss is not None:\n",
    "    print(loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- | Post Training Generate | --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BGL] generate() \n",
      " max_new_tokens :  300\n",
      "[BGL] generate() \n",
      " idx.shape :  torch.Size([1, 1])\n",
      "[BGL] generate() \n",
      " logits.shape :  torch.Size([1, 1, 65])\n",
      "[BGL] generate() \n",
      " idx :  tensor([[0]])\n",
      "[BGL] generate() \n",
      " logits :  tensor([[[ 2.1517e+00, -4.6084e+00, -4.8821e+00, -5.4069e+00, -3.9509e+00,\n",
      "          -7.8209e-01, -3.6401e+00, -4.4815e+00, -4.2010e+00, -2.0870e+00,\n",
      "          -5.8173e+00, -5.0243e+00, -4.3193e+00,  1.3392e+00,  3.5719e-01,\n",
      "           5.4461e-01,  2.2572e-01, -4.8994e-01,  5.4563e-01,  1.2686e-01,\n",
      "          -2.4877e-01,  1.0518e+00, -6.0338e-01, -1.2103e-01,  2.6311e-01,\n",
      "          -6.6172e-02, -1.7088e-01,  2.4896e-01, -1.8473e-04, -9.9460e-01,\n",
      "          -2.0433e-02,  8.3727e-01,  1.3871e+00, -7.9710e-01, -8.2070e-01,\n",
      "           1.0448e+00, -4.3926e+00,  1.0927e-01, -5.6323e+00, -1.1992e+00,\n",
      "          -1.4673e+00, -1.4421e+00, -1.3720e+00, -2.5076e+00, -2.0195e+00,\n",
      "          -1.8701e+00, -1.1588e+00, -1.7623e+00, -2.4397e+00, -2.3857e+00,\n",
      "          -1.6633e+00, -1.2070e+00, -2.1103e+00, -1.1211e+00, -9.0282e-01,\n",
      "          -6.1915e+00, -1.7326e+00, -1.4306e+00, -4.8334e-01, -3.7108e+00,\n",
      "          -2.4386e+00, -9.5122e-01, -3.0207e+00, -1.9147e+00, -5.3306e+00]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "[BGL] generate() \n",
      " logits.shape :  torch.Size([1, 65])\n",
      "[BGL] generate() \n",
      " probs.shape :  torch.Size([1, 65])\n",
      "[BGL] generate() \n",
      " idx.shape :  torch.Size([1, 301])\n",
      "[BGL] generate() \n",
      " idx_next.shape :  torch.Size([1, 1])\n",
      "[BGL] generate() \n",
      " logits :  tensor([[ 2.5166,  0.9509, -0.9272, -0.6108, -2.2813, -1.4093, -2.1913, -0.9734,\n",
      "         -1.1991, -1.5431, -1.8847, -2.8541, -1.4827, -1.3501, -0.0618, -1.2824,\n",
      "         -3.5733, -2.4347, -2.2716, -2.4554, -2.7456, -1.1192, -3.0785, -2.0104,\n",
      "         -1.0397, -2.3826, -3.1391, -2.7686, -0.7029, -2.8102, -2.9909, -2.1059,\n",
      "         -2.2186, -0.0122, -4.6684, -1.6979, -2.9159, -2.3931, -2.1627, -0.2827,\n",
      "         -2.3117, -2.3680, -3.4100, -2.2195, -3.8712, -3.4403, -2.4845, -3.2359,\n",
      "         -2.0339, -2.0138, -1.4761, -1.3266, -3.7552, -0.9655, -1.2841, -1.1915,\n",
      "         -1.5873, -3.3500, -2.7991, -0.1991, -1.6963, -1.0450, -1.9911, -2.4819,\n",
      "         -0.5145]], grad_fn=<SliceBackward0>)\n",
      "[BGL] generate() \n",
      " probs :  tensor([[4.3674e-01, 9.1255e-02, 1.3950e-02, 1.9143e-02, 3.6017e-03, 8.6142e-03,\n",
      "         3.9409e-03, 1.3320e-02, 1.0630e-02, 7.5355e-03, 5.3550e-03, 2.0311e-03,\n",
      "         8.0044e-03, 9.1398e-03, 3.3147e-02, 9.7794e-03, 9.8945e-04, 3.0895e-03,\n",
      "         3.6367e-03, 3.0263e-03, 2.2639e-03, 1.1513e-02, 1.6229e-03, 4.7223e-03,\n",
      "         1.2466e-02, 3.2548e-03, 1.5275e-03, 2.2125e-03, 1.7459e-02, 2.1223e-03,\n",
      "         1.7714e-03, 4.2922e-03, 3.8348e-03, 3.4830e-02, 3.3098e-04, 6.4549e-03,\n",
      "         1.9094e-03, 3.2206e-03, 4.0554e-03, 2.6577e-02, 3.4937e-03, 3.3027e-03,\n",
      "         1.1650e-03, 3.8314e-03, 7.3459e-04, 1.1302e-03, 2.9393e-03, 1.3866e-03,\n",
      "         4.6127e-03, 4.7064e-03, 8.0576e-03, 9.3570e-03, 8.2489e-04, 1.3426e-02,\n",
      "         9.7631e-03, 1.0710e-02, 7.2096e-03, 1.2370e-03, 2.1459e-03, 2.8893e-02,\n",
      "         6.4652e-03, 1.2400e-02, 4.8144e-03, 2.9471e-03, 2.1077e-02]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[BGL] generate() \n",
      " idx :  tensor([[ 0, 32, 13,  1, 43,  1, 46, 56, 39, 50, 57, 59,  2,  0, 21, 58,  2,  0,\n",
      "         15, 17, 39, 50, 43, 57, 54, 56, 43, 59, 54, 56,  1, 57, 39, 47, 58, 46,\n",
      "          1, 50, 42,  1, 39, 41, 59,  1, 13,  1, 50, 39, 58, 46, 43, 56,  0, 30,\n",
      "         19, 17, 30, 47, 57, 54, 56, 56,  1, 63,  1, 58, 46, 47, 43, 58, 46, 47,\n",
      "         49, 11,  1, 24, 13, 16, 35, 21, 10,  1, 58, 46, 47, 52, 45, 50, 53, 52,\n",
      "          1, 47, 58, 46, 47, 52, 42,  1, 39, 58, 41,  8,  0, 21, 52, 10,  0, 48,\n",
      "         12,  0, 53,  1, 44,  1, 46, 43, 56, 43, 56, 56, 42,  1, 56, 42, 47, 45,\n",
      "         50,  1, 63,  1, 57, 53, 51, 51,  1, 42,  1, 44, 43, 58, 46, 39, 50,  1,\n",
      "         51, 43, 39, 52, 57, 58,  1, 54, 54, 50,  1, 58, 58, 46, 39, 52, 43,  1,\n",
      "         31, 53, 59, 45,  1, 50, 57,  1, 58, 46,  1, 59, 52,  6,  0, 13, 16,  9,\n",
      "         36, 17, 56,  1, 58, 47, 42, 57, 43, 58, 46, 43,  1, 52,  6,  1, 53, 56,\n",
      "          1, 42,  1, 19, 24, 19, 20, 13, 49, 63,  2,  0, 19,  4, 54, 39, 60, 43,\n",
      "         43, 50, 58,  1, 54, 53, 44, 53, 58, 43, 57,  1, 42, 43, 52, 42,  1, 58,\n",
      "         53, 59, 10,  0,  0, 13, 17, 34, 59, 57, 54, 43,  0, 31, 28, 23, 21,  5,\n",
      "          1, 58, 46, 39, 56,  1, 54, 56, 43, 52, 50, 39, 47, 45,  6,  1, 39, 56,\n",
      "          1, 61, 45,  1, 61, 53, 46, 39, 63,  1, 51, 39, 52, 42, 57, 58, 53, 40,\n",
      "         53, 59, 32, 46, 43, 43,  1, 15, 20, 10, 28,  2,  1]])\n",
      "[BGL] generate() \n",
      " idx_next :  tensor([[1]])\n",
      "\n",
      "TA e hralsu!\n",
      "It!\n",
      "CEalespreupr saith ld acu A lather\n",
      "RGERisprr y thiethik; LADWI: thinglon ithind atc.\n",
      "In:\n",
      "j?\n",
      "o f hererrd rdigl y somm d fethal meanst ppl tthane Soug ls th un,\n",
      "AD3XEr tidsethe n, or d GLGHAky!\n",
      "G&paveelt pofotes dend tou:\n",
      "\n",
      "AEVuspe\n",
      "SPKI' thar prenlaig, ar wg wohay mandstobouThee CH:P! \n"
     ]
    }
   ],
   "source": [
    "# AFTER training\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# increasing the max_new_tokens in addition to training is starting to get\n",
    "# reasonable output\n",
    "print(decode(m.generate(idx, max_new_tokens=300, debug=True)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- [VERSION 1] -- self-attention toy example\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---VERSION 1--- self-attention toy example\n",
    "torch.manual_seed(1337)\n",
    "# T : a word is stored at each time step/location?\n",
    "# TODO : Verif ^^^\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----x[0]-----\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "----xbow[0]----------------\n",
      "...\n",
      " # @note: 1st row == x[0]\n",
      "...\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "...\n",
      " # rows progressively AVERAGE towards\n",
      " # the SUM of x[0] FULL tensor\n"
     ]
    }
   ],
   "source": [
    "# How do we store the relationship up to the current character, but not beyond?\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "# xbow = bag of words\n",
    "xbow = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # up to AND including current (t) time step\n",
    "        xprev = x[b, : t + 1]  # (t,C)\n",
    "        # mean = average\n",
    "        xbow[b, t] = xprev.mean(dim=0)  # (C)\n",
    "\n",
    "# Verify that each ROW of xbow progressively AVERAGES towards the SUM of x[0]\n",
    "# TODO : is this the KEY to understanding ATTENTION^?\n",
    "# x[0] row[0] ------- row[1] -------  --id(column)--\n",
    "#              0.18           -0.07     0\n",
    "#             -0.35           -0.92     1\n",
    "#              0.63            0.03     2\n",
    "#              0.95            0.06     3\n",
    "#              0.36            1.17     4\n",
    "#             -1.35           -0.51     5\n",
    "#              0.24           -0.24     6\n",
    "#             -0.92            1.54     7\n",
    "# ----------------------------------------------\n",
    "# .18-.35+.63+.95+.36-1.35+.24-.92\n",
    "# -.07-.92+.03+.06+1.17-.51-.24+1.54\n",
    "# ----------------------------------------------\n",
    "# SUMMED   :  -0.26            1.06\n",
    "# AVERAGED :  -0.03            0.13\n",
    "# --- Manually adding and averaging the rows of x[0] ~ xbow[0] ---\n",
    "\n",
    "print(f\"----x[0]-----\\n{x[0]}\")\n",
    "print(\n",
    "    f\"----xbow[0]----------------\\n...\\n # @note: 1st row == x[0]\\n...\\n{xbow[0]}\\n...\\n # rows progressively AVERAGE towards\\n # the SUM of x[0] FULL tensor\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[a]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  1.0  1.0\n",
       "1  1.0  1.0  1.0\n",
       "2  1.0  1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[b]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  2.0  7.0\n",
       "1  6.0  4.0\n",
       "2  6.0  5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[c = a @ b]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  14.0  16.0\n",
       "1  14.0  16.0\n",
       "2  14.0  16.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can speed this up using matrix operations\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "df_table.show_table(a, label='a')\n",
    "df_table.show_table(b, label='b')\n",
    "df_table.show_table(c, label='c = a @ b')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WOW Triangular matrix is essentially a CORNER b&w PIXEL image! WOW\n",
    "- Generates a **`LOWER LEFT TRIANGLE`** matrix of 1s, and flood fills the rest with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[a_tri]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  0.0  0.0\n",
       "1  1.0  1.0  0.0\n",
       "2  1.0  1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[b]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  2.0  7.0\n",
       "1  6.0  4.0\n",
       "2  6.0  5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[a_tri @ b]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0   2.0   7.0\n",
       "1   8.0  11.0\n",
       "2  14.0  16.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[a_tri/ torch.sum(a_tri, 1, keepdim=True)]--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.000000  0.000000\n",
       "1  0.500000  0.500000  0.000000\n",
       "2  0.333333  0.333333  0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tri = torch.tril(torch.ones(3, 3))\n",
    "c = a_tri @ b\n",
    "\n",
    "df_table.show_table(a_tri, label=\"a_tri\")\n",
    "df_table.show_table(b, label='b')\n",
    "df_table.show_table(c, label='a_tri @ b')\n",
    "\n",
    "# @audit üõë : Explain what is happening here\n",
    "a_tri_sum = a_tri / torch.sum(a_tri, 1, keepdim=True)\n",
    "df_table.show_table(a_tri_sum, label=\"a_tri/ torch.sum(a_tri, 1, keepdim=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[starting weights]--\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "--[weighted aggregation]--(summed and averaged)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "print(f\"--[starting weights]--\\n{weights}\\n\")\n",
    "weights = weights / torch.sum(weights, 1, keepdim=True)\n",
    "print(f\"--[weighted aggregation]--(summed and averaged)\\n{weights}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- [VERSION 2] -- loop => matrix weighted aggregation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is xbow and xbow_matrix equal? : True\n",
      "xbow_matrix.shape : torch.Size([4, 8, 2])\n",
      "x.shape : torch.Size([4, 8, 2])\n",
      "xbow_matrix : tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "xbow : tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "x : tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n"
     ]
    }
   ],
   "source": [
    "# ---VERSION 2--- loop => matrix weighted aggregation\n",
    "# (T,T) @ (B,T,C) -----> (B,T,C)\n",
    "# pytorch implicitly adds a B : (B, T,T) @ (B,T,C) -----> (B,T,C)\n",
    "# TODO : understand the above ^^^ and explicitly describe why it works\n",
    "xbow_matrix = weights @ x\n",
    "is_xbow_matrix_equal = torch.allclose(xbow, xbow_matrix)\n",
    "print(f\"is xbow and xbow_matrix equal? : {is_xbow_matrix_equal}\")\n",
    "print(f\"xbow_matrix.shape : {xbow_matrix.shape}\")\n",
    "print(f\"x.shape : {x.shape}\")\n",
    "print(f\"xbow_matrix : {xbow_matrix[0]}\")\n",
    "print(f\"xbow : {xbow[0]}\")\n",
    "print(f\"x : {x[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- [VERSION 3] -- softmax and tril mask\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[tril]--\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "--[w_softmax]--(-inf) \n",
      "# (1) init to zero \n",
      "# (2) use tril as a mask : where 0 == -inf\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "--[w_softmax]--(softmax)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---VERSION 3--- softmax and tril mask\n",
    "tril = torch.tril(\n",
    "    torch.ones(T, T)\n",
    ")  # why did we do this if we were going to zero it out anyways?\n",
    "print(f\"--[tril]--\\n{tril}\\n\")\n",
    "# --- [IMPORTANT AFFINITY] ---\n",
    "# in this toy example we are zeroing it out, but in practice there would be\n",
    "# latent values from previous time (or batch? which is it?) steps\n",
    "# Allows nearest neighbor affinity to be learned\n",
    "# CURRENTLY we are ZEROING it out INLINE, but in PRODUCTION they will LIKELY\n",
    "# have NON-ZERO values when we develop our SELF-ATTENTION BLOCKS\n",
    "# --- [IMPORTANT AFFINITY] ---\n",
    "# - currently we are zeroing it out INLINE\n",
    "# - for self-attention ... TODO : describe what we will do INSTEAD\n",
    "w_softmax = torch.zeros(T, T)\n",
    "# print(f'--[w_softmax]--(zeros)\\n{w_softmax}\\n')\n",
    "\n",
    "# MASKING the future from influencing the past\n",
    "# - decoder : we want to PREDICT the future, can't do that if we KNOW the future\n",
    "# - encoder : DELETE this MASKING step, we want to learn the ENTIRE sequence\n",
    "# TODO : why do we set to -inf?\n",
    "# ANSWER : we are trying to prevent the future from influencing the past\n",
    "# - setting to -inf when [exponentiated] will result in a value of 0\n",
    "# TODO : how did w_softmax go from zero to tril?\n",
    "# ANSWER : Ah we passed in tril as a mask and where == 0 we set to -inf!\n",
    "# -- Tensor.masked_fill_(mask, value)\n",
    "# Fills elements of self tensor with value where mask is True. The shape of mask\n",
    "# must be broadcastable with the shape of the underlying tensor.\n",
    "w_softmax = w_softmax.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(\n",
    "    f\"--[w_softmax]--(-inf) \\n# (1) init to zero \\n# (2) use tril as a mask : where 0 == -inf\\n{w_softmax}\\n\"\n",
    ")\n",
    "# TODO : understand and explain this normalization filter\n",
    "# ANSWER : we are normalizing the values in the matrix to be between 0 and 1\n",
    "#\n",
    "# [w_softmax]        [exponentiated]   [normalized]\n",
    "#\n",
    "# [0, -inf, -inf] => [1, 0, 0]         => [1.0, 0.0, 0.0]\n",
    "# [0, 0, -inf]    => [1, 1, 0]         => [0.5, 0.5, 0.0]\n",
    "# [0, 0, 0]       => [1, 1, 1]         => [0.3, 0.3, 0.3]\n",
    "#\n",
    "#   Softmax is defined as:\n",
    "#   :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "#   It is applied to all slices along dim, and will re-scale them so that the\n",
    "#   elements lie in the range `[0, 1]` and sum to 1.\n",
    "# TODO : what does it mean to be exponentiated?\n",
    "# ANSWER : exponentiated means to raise to the power of e\n",
    "#          e is the base of the natural logarithm\n",
    "# TODO : understand what it means to raise to the power of e\n",
    "# ANSWER : When [exponentiated] and then [normalized] :\n",
    "# [exponentiated] : [0, 0, -inf]    => [1, 1, 0]\n",
    "# - raised to the power of 0 == 1\n",
    "# - raised to the power of -inf == 0\n",
    "# [normalized] : [0.5, 0.5, 0.0]\n",
    "w_softmax = F.softmax(w_softmax, dim=-1)\n",
    "print(f\"--[w_softmax]--(softmax)\\n{w_softmax}\\n\")\n",
    "# sum and aggregate wrt affinity -- effectively a simple average between each\n",
    "# token/character and it's previous neighboring characters up to current token\n",
    "xbow_softmax = w_softmax @ x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0700])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow_softmax[0][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- [VERSION 4] -- Self-Attention\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--[self_attention[0]]-- (softmax) => torch.Size([8, 8])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "--[out[0]]-- (w_self_attention @ v) => torch.Size([8, 16])\n",
      "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
      "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
      "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
      "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
      "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
      "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
      "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
      "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
      "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
      "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
      "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
      "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
      "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
      "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
      "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
      "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ---VERSION 4--- Self-Attention\n",
    "torch.manual_seed(1337)\n",
    "# @audit-ok üîé : explain why channel size is 32\n",
    "# in image space the equivalent would be ~ :\n",
    "# - B,T => would be width, height (x,y pixels)\n",
    "# - C => Grayscale (1) or RGB (3) or RGBA (4)\n",
    "B, T, C = 4, 8, 32  # batch (pools:4), time (nodes:8), channels (features?:32)\n",
    "# initialize x as a random tensor of shape (B,T,C) => torch.Size([4, 8, 32])\n",
    "x = torch.randn(B, T, C)\n",
    "# print(f'--[x[0][0]]-- (init as random tensor) => {x.shape}\\n{x[0][0]}')\n",
    "# @audit ‚äπ --[x[0][0]]-- (init as random tensor) => torch.Size([4, 8, 32])\n",
    "\n",
    "# @note Let's see a single head perform self-attention\n",
    "head_size = 16  # hyperparameter\n",
    "# @audit-ok ‚òë : explain why we need 3 nn.Linear functions init, can we do it in 1?\n",
    "# nn.Linear(in_features, out_features, bias=True) :\n",
    "# - Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\"\"\" \n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "\"\"\"\n",
    "# ANSWER : There is NO communication between the key, query, and value vectors\n",
    "# so we need 3 separate nn.Linear functions to capture each INDEPENDENT vector\n",
    "# @audit üõë : why nn.Linear to convert @x from shape (B,T,C) to (B,T,head_size)?\n",
    "# - out_x.shape : torch.Size([4, 8, 32]) out_v.shape : torch.Size([4, 8, 16])\n",
    "# @note : Every token/node emits two vectors : key and query\n",
    "# @note : \"affinity\" is the dot product of the query and key vectors\n",
    "#         the more the query and key are ALIGNED the higher the affinity\n",
    "# @note : key vector captures \"what do I contain?\"\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# @note : query vector captures \"what am I looking for?\"\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# @note : value vector captures \"what do I need to know?\"\n",
    "# @audit-ok üîé : explain why we need value vector ... is it something that comes\n",
    "# into play when we have multiple heads?\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B,T,16 (head_size))\n",
    "# @audit ‚äπ : nn.Linear(x) is tensor size [4,8,32]\n",
    "# print(f'--[k]-- => {k[0].shape}\\n{k[0]} ')\n",
    "# @audit üñ∂ --[k]-- => torch.Size([8, 16])\n",
    "\"\"\" \n",
    "tensor([[ 0.1196, -0.3013,  0.3629,  1.1771,  1.1385, -0.2554,  0.1454, -0.2944,\n",
    "         -0.7020, -1.0308,  0.7436, -0.8098, -0.6669,  0.0912, -0.0061,  0.1983],\n",
    "        [-0.5423, -0.5558, -0.0761,  1.2929,  0.8653, -1.1998,  0.3878,  0.1939,\n",
    "          0.7024, -0.8225,  0.2348, -0.8499, -0.3813, -0.2991,  0.0102, -0.5545],\n",
    "        [-0.3736, -0.4678, -0.2156, -0.8034, -0.3715, -0.5443, -0.9146, -0.0559,\n",
    "         -0.3290, -0.2102,  0.1166, -0.1798, -0.2820, -0.3320, -0.4596, -0.1325],\n",
    "        [-0.3146,  0.0845, -0.1235, -0.7058, -0.1802,  0.5492, -0.8980, -0.4938,\n",
    "          0.6791,  0.8827,  0.4911,  0.5190,  0.9011,  0.0913, -0.1933, -0.6770],\n",
    "        [ 0.0239,  0.0998, -0.1871, -0.0860, -0.4881, -1.6765,  0.2413,  0.7361,\n",
    "          0.4608, -0.8722, -0.4259, -1.1347, -1.0571, -0.9401,  0.1343, -0.0157],\n",
    "        [-0.2362, -0.7873, -0.3802,  0.5815, -0.3722,  1.2405, -0.7004, -1.4917,\n",
    "          0.7678,  0.3584,  0.6120, -0.0794,  0.5983,  0.2635,  0.6490,  0.0709],\n",
    "        [-0.7941, -0.1660, -0.2810, -0.1021, -0.7352, -0.7518, -0.1276, -0.0051,\n",
    "          0.3325, -0.3374,  0.1678,  0.3105,  0.2258,  0.1243,  0.4617,  0.2016],\n",
    "        [ 0.1651, -0.1599, -0.5717, -0.3957,  0.3930, -0.8567,  0.3390, -0.7977,\n",
    "          0.2213, -0.5161,  0.1850, -0.2105,  0.3779,  0.0482, -0.4744, -0.0504]],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "\"\"\"\n",
    "# print(f'--[k.transpose]-- => {k[0].transpose(-2,-1).shape}\\n{k[0].transpose(-2,-1)}')\n",
    "# @audit üñ∂ --[k.transpose]-- => torch.Size([16, 8])\n",
    "# @audit-ok üîé explain k[0].transpose(-2, -1) **[16,8]** transformed from  k[0] = **[8,16]**\n",
    "\"\"\"\n",
    "tensor([[ 0.1196, -0.5423, -0.3736, -0.3146,  0.0239, -0.2362, -0.7941,  0.1651],\n",
    "        ...\n",
    "       grad_fn=<TransposeBackward0>)\n",
    "\"\"\"\n",
    "# @audit : **[grad_fn=<TransposeBackward0>)]** from **[grad_fn=<SelectBackward0>]**\n",
    "q = query(x)  # (B,T,16 (head_size))\n",
    "# print(f'--[q]-- => {q[0].shape}\\n{q[0]}')\n",
    "# @audit üñ∂ --[q]-- => torch.Size([8, 16])\n",
    "\"\"\" \n",
    "tensor([[-0.6567,  0.0283,  0.0094, -0.6995, -0.3604,  0.8376, -0.4446,  0.1228,\n",
    "          0.6276, -0.6222,  0.3483,  0.2411,  0.5409, -0.2605,  0.3612, -0.0436],\n",
    "        [-0.3932,  0.8220, -0.7027,  0.0954, -0.1222, -0.1518, -0.5024, -0.4636,\n",
    "          0.1176,  1.4282, -0.5812,  0.1401,  0.9604,  0.0410, -0.6214, -0.6347],\n",
    "        [ 0.2157, -0.3507,  0.0022,  0.4232, -0.2284, -0.0732, -0.3412,  0.9647,\n",
    "         -0.5178,  0.0921, -0.5043,  0.8388,  0.6149, -0.0109, -0.5569,  0.5820],\n",
    "        [ 0.9000, -0.1272,  0.5458,  0.4254, -0.4513, -0.0212,  0.1711,  0.2599,\n",
    "         -0.9978,  0.4890,  0.1737, -0.0700, -0.3113,  0.3748, -0.1848, -0.6379],\n",
    "        [ 0.0332,  0.5886, -0.4437,  0.3775, -0.6826, -0.2775,  0.4673, -1.2956,\n",
    "          0.6603,  0.1633, -1.7573, -0.6582, -0.2302, -0.0862, -0.0060,  0.7573],\n",
    "        [ 0.2098,  0.0439, -0.0702,  0.0727, -0.2012, -1.7539,  1.0369,  0.1163,\n",
    "          0.2956,  0.3231,  0.5052,  0.7011, -0.2844, -0.7844,  0.4782, -0.5170],\n",
    "        [ 0.6100, -0.3284, -0.8557,  0.8543,  0.7805, -0.4023, -0.8183, -0.0554,\n",
    "          0.1873,  0.2706, -0.7066, -0.8637,  0.6998, -0.0670,  0.2551,  0.2149],\n",
    "        [ 0.1459,  0.1349, -0.2335, -0.0417,  0.2928, -0.5080,  0.1177,  0.1861,\n",
    "          0.1455,  0.0292, -0.8470,  0.6116,  1.2445,  0.1909,  0.3694, -0.0027]],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "\"\"\"\n",
    "# TODO : dot product between query (search) and key (data) somehow results in attention?\n",
    "# ANSWER : ATTENTION is the DOT PRODUCT of query and key\n",
    "# @note : FINALLY we compute ATTENTION => the dot product of query and key\n",
    "w_self_attention = q @ k.transpose(-2, -1)  # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "# print(f'--[self_attention[0]]-- (q @ k) => {w_self_attention[0].shape}\\n{w_self_attention[0]}')\n",
    "# @audit üñ∂ --[self_attention[0]]-- (q @ k) => torch.Size([8, 8])\n",
    "\"\"\"\n",
    "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
    "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
    "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
    "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
    "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
    "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
    "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
    "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "\"\"\"\n",
    "\n",
    "# create a triangle mask of shape (T,T)) => torch.Size([8, 8])\n",
    "\"\"\"\n",
    "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "\"\"\"\n",
    "# @audit-ok üîé : explain why is the dimension of the mask (T,T) and not (B,T,T) ?\n",
    "# - is it because T (8) is larger than B (4) for tensor BROADCASING ?\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# @audit-ok ‚òë : DO NOT ZERO it out, else we are simpling AVERAGE per row, effectively\n",
    "#         preventing building up ATTENTION in a data dependent way\n",
    "# w_self_attention = torch.zeros(T,T)\n",
    "\n",
    "# @note : mask prevents future from influencing the past for DECODING\n",
    "# - we are trying to build a PREDICTIVE model, so masking the future makes sense\n",
    "# - but... if this were SENTIMENT ANALYSIS, we would NOT want to mask the future\n",
    "#   for that use case we may elect to include the future in the attention weights\n",
    "# ENCODER block, DELETE this mask and allow all nodes to talke to each other\n",
    "w_self_attention = w_self_attention.masked_fill(tril == 0, float(\"-inf\"))\n",
    "# print(f'--[self_attention[0]]-- (masked_fill) => {w_self_attention[0].shape}\\n{w_self_attention[0]}')\n",
    "# --[self_attention[0]]-- (masked_fill) => torch.Size([8, 8])\n",
    "\"\"\"\n",
    "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
    "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
    "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
    "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
    "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
    "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
    "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
    "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "\"\"\"\n",
    "# @audit-ok üîé : explain why is dim -1?\n",
    "w_self_attention = F.softmax(w_self_attention, dim=-1)\n",
    "print(\n",
    "    f\"--[self_attention[0]]-- (softmax) => {w_self_attention[0].shape}\\n{w_self_attention[0]}\"\n",
    ")\n",
    "# @audit üñ∂ --[self_attention[0]]-- (softmax) => torch.Size([8, 8])\n",
    "# @audit-ok ‚òë\n",
    "# - tril mask ‚úã => rows don't CHEAT read at the FUTURE, N row stopped to N column\n",
    "# - softmax üí¨ => each row sums to 1.0\n",
    "\"\"\"\n",
    "tensor([[1.0000 ‚úã, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]    , üí¨\n",
    "        [0.1574, 0.8426 ‚úã, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]    , üí¨\n",
    "        [0.2088, 0.1646, 0.6266 ‚úã, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]    , üí¨\n",
    "        [0.5792, 0.1187, 0.1889, 0.1131 ‚úã, 0.0000, 0.0000, 0.0000, 0.0000]    , üí¨\n",
    "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909 ‚úã, 0.0000, 0.0000, 0.0000]    , üí¨\n",
    "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019 ‚úã, 0.0000, 0.0000]    , üí¨\n",
    "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329 ‚úã, 0.0000]    , üí¨\n",
    "        [0.0210, 0.0843, 0.0555, 0.2297üëÄ, 0.0573, 0.0709, 0.2423, 0.2391üëÄ ‚úã]], üí¨\n",
    "       grad_fn=<SelectBackward0>)\n",
    "\n",
    "# @audit-ok üîé : Let's look üëÄ at the eigth token of self_attention[0]\n",
    "# - üëÄ [8th token] is [0.2391]\n",
    "# -[0.2297] üëÄ has high affinity and is interesting to the [8th token]\n",
    "# @audit üõë ... !!! I STILL DON'T GET IT üòìüò∞üòÖ !!!\n",
    "\"\"\"\n",
    "\n",
    "# out = w_self_attention @ x\n",
    "# audit-ok üîé : explain how @ x is looking up PRIVATE information, but somehow\n",
    "# @ v aggregates it?\n",
    "# @audit üõë : What ... the tensor shape changes from [4, 8, 32] to [4, 8, 16] when we do @ v\n",
    "# out_x.shape : torch.Size([4, 8, 32]) out_v.shape : torch.Size([4, 8, 16])\n",
    "v = value(x)\n",
    "out = w_self_attention @ v\n",
    "# @audit üñ∂ --[out[0]]-- (w_self_attention @ v) => torch.Size([8, 16])\n",
    "\"\"\"\n",
    "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
    "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
    "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
    "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
    "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
    "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
    "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
    "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
    "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
    "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
    "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
    "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
    "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
    "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
    "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
    "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
    "       grad_fn=<SelectBackward0>)\n",
    "\"\"\"\n",
    "print(f\"--[out[0]]-- (w_self_attention @ v) => {out[0].shape}\\n{out[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_x.shape : torch.Size([4, 8, 32]) out_v.shape : torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "out_x = w_self_attention @ x\n",
    "out_v = w_self_attention @ v\n",
    "print(f\"out_x.shape : {out_x.shape} out_v.shape : {out_v.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- [FOOTNOTES] --\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes :\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space.  Attention simply acts over a set of vectors.  This is why we need to positionally encode tokens.  `# TODO - is CONVOLUTION an IMPLICIT attention mechanism, where nearest neighbors are the IMPLICIT positional embedding`\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries.  In \"cross-attention\" the `???` get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- In an \"encoder\" attention block just **delete the single line that does masking** with `tril`, allowing all tokens to communicate. `???` here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings like `???` modeling.\n",
    "- \"Scaled\" attention divides `wei` by 1/sqrt(`head_size`).  This makes is so when input Q,K are unit variance, `wei` will be `???` too and Softmax will stay diffuse and not saturate too much. Illustration below `???`\n",
    "\n",
    "##### Explain Footnotes ^ like I am 5 \n",
    "\n",
    "Okay, let's break it down into simpler terms:\n",
    "\n",
    "1. **Attention**: Imagine you're in a classroom where everyone is talking at once, and you want to focus on what your best friend is saying. You would \"pay more attention\" to the sounds coming from your friend, and less to the other noise. That's what the attention mechanism does - it pays more attention to the important parts of the data and less to the less important parts.\n",
    "\n",
    "2. **No notion of space**: Attention doesn't care about where things are in relation to each other. Imagine you are playing with a set of different toys - a car, a doll, a ball. You don't care where these toys are placed, you just play with each toy based on how much you like it. That's why we need to tell the model about the order of words in a sentence, because it doesn't understand this on its own.\n",
    "\n",
    "3. **Each example is processed independently**: Think of it like doing your homework - you do each problem on its own, without mixing up the information from different problems.\n",
    "\n",
    "4. **Self-attention and cross-attention**: Let's say you're trying to understand a story. If you think about the story and come up with questions and answers all by yourself, that's like self-attention. But if you get help from your teacher or a book (an external source) to understand the story, that's like cross-attention.\n",
    "\n",
    "5. **Encoder and decoder**: Imagine you're playing a game of telephone. The person who whispers the message into the next person's ear is like the encoder, and the person who tries to understand and pass along the message is like the decoder. In some cases, we allow all the players to talk to each other freely (like the encoder), and in others we only allow each player to hear from the players who spoke before them (like the decoder).\n",
    "\n",
    "6. **Scaled attention**: Imagine you're sharing a pie with your friends. If you divide the pie evenly among everyone, each person only gets a small piece. But if you give a bigger piece to your best friend, you would have to make the other pieces smaller. Similarly, in scaled attention, we divide the attention \"pie\" among all the words in a sentence, but we give bigger \"pieces\" to the more important words.\n",
    "\n",
    "The parts with `???` are placeholders for information that should be filled in, based on the specific context or model that is being discussed.\n",
    "\n",
    "##### Can you explain Self-attention and cross-attention\n",
    "\n",
    "Sure, let's break those down:\n",
    "\n",
    "**Self-attention:**\n",
    "\n",
    "Self-attention, also sometimes called intra-attention, is a mechanism that allows each token in an input sequence to look at the other tokens in the same sequence to get better context and thereby improve the representation of the sequence. \n",
    "\n",
    "Here's a simplified explanation: Imagine you're reading a book and you come across a sentence that says, \"Even though he was very tired, John decided to continue reading the book because it was so interesting.\" Now, if you want to understand who \"he\" is referring to in this sentence, you'll look at the other parts of the sentence, and realize that \"he\" is referring to \"John\". This is similar to what self-attention does - it allows a token to look at the other tokens in the sequence to get context and understand better.\n",
    "\n",
    "**Cross-attention:**\n",
    "\n",
    "Cross-attention, on the other hand, is a mechanism that allows each token in an input sequence to interact with tokens in a separate sequence. This is often used in tasks that involve pairs of sentences, like question answering or machine translation, where the model needs to understand the relationship between two different sequences.\n",
    "\n",
    "Let's take another simplified example: Imagine you're translating a sentence from English to Spanish. You come across the English word \"book\", which can be translated into Spanish as \"libro\". However, the exact translation might depend on the other words in the English sentence and their Spanish counterparts. So, you need to look at both the English sentence and the Spanish sentence you're constructing to decide on the best translation. This is similar to what cross-attention does - it allows a token in one sequence to look at the tokens in a different sequence to get additional context and improve the translation.\n",
    "\n",
    "In summary, self-attention and cross-attention are mechanisms that allow tokens to interact with each other to get better context and improve the model's understanding of the data. The difference between them lies in whether the tokens are looking at other tokens in the same sequence (self-attention) or a different sequence (cross-attention).\n",
    "\n",
    "![Markdown Logo is here.](https://www.fullstackpython.com/img/logos/markdown.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- KEY INSIGHTS --\n",
    "\n",
    "##### Building a GPT from Scratch - Guide with Code & Notebooks\n",
    "\n",
    "###### Technical aspects of GPT and Transformer architecture\n",
    "\n",
    "- üß† The Transformer architecture, originally proposed for machine translation,  \n",
    "ended up taking over the rest of AI in the next five years after its publication.\n",
    "- ü§ñ The Transformer neural network can model patterns in data, such as  \n",
    "Shakespeare's works, and generate infinite sequences that look like  \n",
    "Shakespeare's language.\n",
    "- üíª When training a Transformer, chunks of data are sampled and trained on  \n",
    "individually to make predictions at every position, allowing the network to be  \n",
    "used to seeing `contexts` from as little as `one` all the way up `to block size`\n",
    "- üß† The `negative log likelihood loss`, also known as `cross entropy`, is a  \n",
    "good way to measure the quality of predictions in language modeling.\n",
    "- üßê GPT's approach of only allowing information to flow from previous context  \n",
    "to the current timestamp means that it cannot get any information from the  \n",
    "future, which is necessary for predicting the future accurately. (tri-mask üìê)\n",
    "- üßê Understanding self-attention is the most important part of building GPT  \n",
    "from scratch.\n",
    "- üí° ==**Self-attention**== solves the problem of gathering information from the  \n",
    "past in a data-dependent way, allowing tokens to interact and learn more about  \n",
    "specific tokens in the sequence.\n",
    "- ü§Ø The self-attention mechanism in GPT involves parallel and independent  \n",
    "production of keys and queries for all tokens, followed by dot product to  \n",
    "generate affinities and weighted aggregation in a data-dependent manner.\n",
    "- ü§ñ Scaling up the neural net with larger `batch size`, `block size`,  \n",
    "`embedding dimension`, and `more layers` can significantly improve its  \n",
    "`performance`, but it requires a powerful `GPU` for training.\n",
    "- ü§ñ The size of GPT-3's largest Transformer, with 175 billion parameters, is a  \n",
    "massive infrastructure challenge to train, requiring thousands of GPUs to talk  \n",
    "to each other.\n",
    "- Implications and potential impact of AI-powered natural language interfaces\n",
    "    - üí¨ GPT can generate humorous and unexpected responses to prompts, such as  \n",
    "    explaining HTML to a dog or writing release notes for chess 2.\n",
    "- ü§ñ The process of converting logits to probabilities via softmax and sampling  \n",
    "from those probabilities is crucial in generating predictions for what comes  \n",
    "next in AI-powered natural language interfaces.\n",
    "- ü§ñ The use of AI-powered natural language interfaces could potentially change  \n",
    "the paradigm of programming, allowing ordinary people to get complex tasks done  \n",
    "without needing programming expertise.\n",
    "\n",
    "https://eightify.app/summary/data-science-and-ai/create-your-own-gpt-step-by-step-guide-with-code-notebooks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between numpy and panda\n",
    "***\n",
    "\n",
    "`numpy` and `pandas` are both popular Python libraries used in data analysis and  \n",
    "manipulation, but they serve different purposes and have different strengths.\n",
    "\n",
    "**NumPy (Numerical Python):**\n",
    "\n",
    "- NumPy is a fundamental library for numerical computing in Python. It provides  \n",
    "support for arrays, matrices, and mathematical functions that operate on these  \n",
    "data structures.\n",
    "- The core of NumPy is the `ndarray` (n-dimensional array) object, which is a  \n",
    "multi-dimensional, homogeneous, and fixed-size array.\n",
    "- NumPy is designed for efficient numerical computations, and it provides a wide  \n",
    "range of mathematical functions such as linear algebra, statistical operations,  \n",
    "and element-wise operations.\n",
    "- `NumPy` is the `foundation for` many other scientific and data analysis libraries  \n",
    "in Python, including **`pandas`**, SciPy, and scikit-learn.\n",
    "\n",
    "**pandas (Python Data Analysis Library):**\n",
    "\n",
    "- `pandas` is a library built on top of NumPy that provides data manipulation and  \n",
    "analysis tools specifically designed for working with structured data, such as  \n",
    "**`tables`** or **`time series`**.\n",
    "- The main data structures in pandas are the `Series` (1-dimensional) and  \n",
    "`DataFrame` (2-dimensional) objects. These data structures are built on top of  \n",
    "NumPy arrays but provide additional functionality for handling missing data,  \n",
    "data alignment, and working with heterogeneous data types.\n",
    "- `pandas is well-suited for` handling real-world datasets, as it provides  \n",
    "powerful tools for **`reading and writing data`** in various formats such as :\n",
    "    - CSV   \n",
    "    - Excel     \n",
    "    - JSON  \n",
    "    - SQL databases  \n",
    "- pandas includes many functions for data cleaning, transformation, aggregation,  \n",
    "and visualization, making it a popular choice for data preprocessing and  \n",
    "exploratory data analysis.\n",
    "\n",
    "In summary, NumPy is a foundational library for numerical computing that focuses  \n",
    "on providing efficient array and matrix operations, while pandas is a  \n",
    "higher-level library specifically designed for data manipulation and analysis of  \n",
    "structured data.  \n",
    "\n",
    "While both libraries are essential in the data processing pipeline, you would  \n",
    "typically use NumPy for lower-level mathematical operations and pandas for  \n",
    "handling real-world datasets and performing data cleaning and analysis tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between .view() and .reshape()\n",
    "***\n",
    "\n",
    "In the context of NumPy, both `.view()` and `.reshape()` are used to change the  \n",
    "shape of an array. However, they operate in different ways:\n",
    "\n",
    "**view()**\n",
    "\n",
    "`.view()` returns a new array object that shares the same data with the original  \n",
    "array. It can also be used to change the data type of the array, which is not  \n",
    "possible with `.reshape()`. However, the main difference is that `.view()` will  \n",
    "not guarantee contiguous memory layout, meaning that it can return a view of the  \n",
    "original array even if the new shape would require a different memory layout.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "\n",
    "new_arr = arr.view(dtype=np.float64)\n",
    "```\n",
    "\n",
    "In this case, `new_arr` will have the same data as `arr`, but the data type will  \n",
    "be changed to `float64`.\n",
    "\n",
    "**reshape()**\n",
    "\n",
    "`.reshape()` returns a new array object with a modified shape. Unlike `.view()`,  \n",
    "it only changes the shape of the array, without affecting the data type. If it's  \n",
    "possible to create the new shape without changing the memory layout, `.reshape()`  \n",
    "will return a view of the original array. Otherwise, it will return a copy of  \n",
    "the original array with the new shape.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "new_arr = arr.reshape((2, 3))\n",
    "```\n",
    "\n",
    "In this case, `new_arr` will be a 2x3 array containing the same data as `arr`.\n",
    "\n",
    "In summary, `.view()` can change both the shape and the data type of an array  \n",
    "while sharing the same data, while `.reshape()` only changes the shape of the  \n",
    "array. However, while both methods can return views of the original array,  \n",
    "`.reshape()` might return a copy if the new shape requires a different memory  \n",
    "layout."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the context where I would use .view() vs .reshape()\n",
    "***\n",
    "\n",
    "Here are the contexts in which you would choose to use `.view()` or `.reshape()`:\n",
    "\n",
    "**Use `.view()` when:**\n",
    "\n",
    "1. You want to change the memory layout or data type of the array without making  \n",
    "a copy of the data. For example, if you have an array of `int32` values, and you  \n",
    "want to view them as `float64` values, you can use `.view()` to achieve this  \n",
    "without copying the data.\n",
    "   \n",
    "   ````python\n",
    "   import numpy as np\n",
    "   \n",
    "   arr = np.array([1, 2, 3, 4], dtype=np.int32)\n",
    "   float_view = arr.view(dtype=np.float64)\n",
    "   ```\n",
    "\n",
    "2. You want to create a new array that shares the same data with the original  \n",
    "array, but you do not need to guarantee that the new array will have a contiguous  \n",
    "memory layout. This is useful when you want to modify the new array without  \n",
    "affecting the original array or when you want to work with different views of  \n",
    "the same data.\n",
    "\n",
    "   ````python\n",
    "   import numpy as np\n",
    "   \n",
    "   arr = np.array([[1, 2], [3, 4]])\n",
    "   transposed_view = arr.T.view()\n",
    "   ```\n",
    "\n",
    "**Use `.reshape()` when:**\n",
    "\n",
    "1. You want to change the shape of an array while keeping its data type unchanged.  \n",
    "This is useful when you want to transform a 1D array into a 2D array, for example,  \n",
    "or when you want to change the dimensions of a multi-dimensional array.\n",
    "\n",
    "   ````python\n",
    "   import numpy as np\n",
    "   \n",
    "   arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "   reshaped_arr = arr.reshape((2, 3))\n",
    "   ```\n",
    "  \n",
    "2. You want to create a new array with a different shape, and you want to ensure  \n",
    "that the new array has a contiguous memory layout. In cases where the desired shape  \n",
    "cannot be created with a contiguous layout, `.reshape()` will return a copy of the  \n",
    "original array with the new shape.\n",
    "\n",
    "   ````python\n",
    "   import numpy as np\n",
    "   \n",
    "   arr = np.array([[1, 2], [3, 4]])\n",
    "   flattened_arr = arr.reshape(-1)\n",
    "   ```\n",
    "\n",
    "In summary, use `.view()` when you want to change the memory layout or data type  \n",
    "without copying the data or when you do not need to guarantee contiguous memory  \n",
    "layout. Use `.reshape()` when you want to change the shape of an array while  \n",
    "keeping its data type the same and ensuring a contiguous memory layout if possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk me through Tensor reshape\n",
    "***\n",
    "\n",
    "Tensor reshaping is the process of changing the shape of a tensor without  \n",
    "altering its underlying data. This is useful when you need to change the  \n",
    "dimensions of a tensor for certain operations or when you need to rearrange the  \n",
    "data to a different shape for further processing.\n",
    "\n",
    "Here's a step-by-step guide to reshaping tensors using the `reshape` method in  \n",
    "PyTorch:\n",
    "\n",
    "1. **Import the necessary library**\n",
    "\n",
    "   First, import the PyTorch library, which provides the `torch` module for  \n",
    "   working with tensors:\n",
    "\n",
    "   ````python\n",
    "   import torch\n",
    "   ```\n",
    "\n",
    "2. **Create a tensor**\n",
    "\n",
    "   Create a tensor with the initial shape. For example, let's create a 1D tensor  \n",
    "   with 12 elements:\n",
    "\n",
    "   ````python\n",
    "   initial_tensor = torch.arange(12)\n",
    "   print(initial_tensor)\n",
    "   # Output: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
    "   ```\n",
    "\n",
    "3. **Reshape the tensor**\n",
    "\n",
    "   Now, use the `reshape` method to change the shape of the tensor. For  \n",
    "   instance, let's reshape the 1D tensor into a 3x4 matrix:\n",
    "\n",
    "   ````python\n",
    "   reshaped_tensor = initial_tensor.reshape(3, 4)\n",
    "   print(reshaped_tensor)\n",
    "   # Output:\n",
    "   # tensor([[ 0,  1,  2,  3],\n",
    "   #         [ 4,  5,  6,  7],\n",
    "   #         [ 8,  9, 10, 11]])\n",
    "   ```\n",
    "\n",
    "   The `reshape` method takes the desired dimensions as arguments and returns a  \n",
    "   new tensor with the specified shape. Make sure that the product of the new  \n",
    "   dimensions is equal to the total number of elements in the initial tensor.  \n",
    "   In this case, 3 * 4 = 12.\n",
    "\n",
    "4. **Using `-1` as a dimension**\n",
    "\n",
    "   You can use `-1` as one of the dimensions in the `reshape` method to  \n",
    "   automatically infer the size of that dimension based on the size of the other  \n",
    "   dimensions and the total number of elements in the tensor. For example:\n",
    "\n",
    "   ````python\n",
    "   reshaped_tensor = initial_tensor.reshape(3, -1)\n",
    "   print(reshaped_tensor)\n",
    "   # Output:\n",
    "   # tensor([[ 0,  1,  2,  3],\n",
    "   #         [ 4,  5,  6,  7],\n",
    "   #         [ 8,  9, 10, 11]])\n",
    "   ```\n",
    "\n",
    "   In this case, the `-1` is replaced with 4, as it's the only value that  \n",
    "   ensures the product of dimensions is equal to the total number of elements (12).\n",
    "\n",
    "That's it! With these steps, you can reshape tensors in PyTorch to fit your needs.  \n",
    "Remember that reshaping doesn't modify the original tensor's data, only its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6] \n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "reshaped_arr = arr.reshape((2, 3))\n",
    "print(f\"{arr} \\n{reshaped_arr}\")\n",
    "reshaped_arr.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "initial_tensor = torch.arange(12)\n",
    "print(initial_tensor)\n",
    "reshaped_tensor = initial_tensor.reshape(3, -1)\n",
    "print(reshaped_tensor)\n",
    "transposed_tensor = reshaped_tensor.transpose(0, 1)\n",
    "print(transposed_tensor)\n",
    "squeezed_tensor = transposed_tensor.squeeze()\n",
    "print(squeezed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000, 6.0000, 7.0000, 8.0000],\n",
       "        [1.1000, 2.1000, 3.1000, 4.1000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([[5.0, 6.0, 7.0, 8.0], [1.1, 2.1, 3.1, 4.1]])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0321, 0.0871, 0.2369, 0.6439],\n",
       "        [0.0321, 0.0871, 0.2369, 0.6439]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0560, 0.4175, 0.3114, 0.9418],\n",
       "        [0.9193, 0.2046, 0.6022, 0.1689]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_logit = torch.rand(2, 4)\n",
    "random_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0560, 0.4175, 0.3114, 0.9418],\n",
       "        [0.9193, 0.1046, 0.0022, 0.0089]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_logit = torch.tensor([\n",
    "    [0.0560, 0.4175, 0.3114, 0.9418],\n",
    "    [0.9193, 0.1046, 0.0022, 0.0089]])\n",
    "random_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1626, 0.2334, 0.2099, 0.3942],\n",
       "        [0.4455, 0.1972, 0.1780, 0.1792]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_probs = F.softmax(random_logit, dim=1)\n",
    "random_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0203b3654eb8bc60da03d36108deed569263a68c1ed718ab77fdacb38634fa76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
